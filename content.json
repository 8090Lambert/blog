{"meta":{"title":"8090Lambert | Blog","subtitle":"A Programmer With Coding.","description":"A Programmer With Coding.","author":"8090Lambert","url":"http://8090lambert.cn"},"pages":[{"title":"关于","date":"2016-10-16T08:03:30.000Z","updated":"2021-05-09T14:50:16.600Z","comments":true,"path":"about/index.html","permalink":"http://8090lambert.cn/about/index.html","excerpt":"","text":"About Me 男 &nbsp; 90后 后端开发工程师，热爱技术，设计模式重度患者，有轻微代码洁癖 热衷研究实现原理，系统集群架构，分布式存储数据同步 最大同性交友平台ID ：8090Lambert"},{"title":"文章标签","date":"2018-10-28T09:47:57.000Z","updated":"2021-05-09T14:50:16.630Z","comments":false,"path":"tags/index.html","permalink":"http://8090lambert.cn/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"LSMTree结构理解","slug":"LSMTree结构理解","date":"2021-08-03T09:35:42.000Z","updated":"2021-11-04T09:58:13.279Z","comments":true,"path":"2021/08/03/LSMTree结构理解/","link":"","permalink":"http://8090lambert.cn/2021/08/03/LSMTree结构理解/","excerpt":"","text":"LSM-Tree 结构的存储系统，将离散的随机写请求，通过WAL + Compaction机制变为批量的顺序写请求，提升写入性能。但是，多个level、每层多个SST文件的架构也存在一些问题： 读放大：当查询一个key时，需要遵循从新到旧的查找过程，直到找到想要的数据。虽然有全局manifest文件索引、每个SSTable还有Bloom filter，但是逐层的步骤不可避免，这个过程可能需要不止一次的磁盘IO，尤其是做query range时，影响更明显； 空间放大：所有写入都是顺序写，修改和删除操作也是通过写入新的 SN 来实现，所以无效数据不会随着delete一起立即删除，而是在compaction时实现物理删除； 写放大：每层level，在SSTable数量到达一定阈值时，会发生compation操作（本层和下一层中的相同key如果有重叠，会进行排序后的重写） 整体结构每一个 Record 由 &lt;Key, SN(Sequence Number), ValueType, Value&gt; 四部分组成。SN 是在写入数据时生成的一个全局递增的整数，ValueType 表明这个 Record 是否为有效的 Record。 Record 被分为多层保存，从 Level 0 开始到 Level N。除 Level 0 外，每一个 Level 内的 Record 都是有序的，且被分别保存在多个文件中，每个文件被称作一个 SStable。 写入流程： 生成一个新的SN. 将 Record 写入WAL（write ahead log），保证Crash Safe(Consistency Safe). 将 Record 写入内存中的 Memtable. 当 MemTable(可读可写) 中的数据量超过某个值(一般4MB)时，会变为 Immutable memtable(可读)。通过双buffer的机制，有效减少了 MemTable 在写满时，由于flush到磁盘可能产生的阻塞问题。 查询流程一般读取有两种方式： 通过get接口读取数据. 创建一个snapshot，基于该snapshot调用get接口读取数据. 这两种方式本质一样，都是快照读，第一种方式会先隐式的创建一个数据库当前状态的快照，然后基于这个快照再去调用get方法读. 具体的步骤如下： 在Memtable（skipList）中查找指定的key，若搜索到符合条件的key就结束查找； 在Immutable memtable（skipList）中查找指定的key，若搜索到符合条件的key就结束查找； 按照level0 -&gt; levelN的顺序在每层中的sstable文件中查找指定的key，若搜索到符合条件的数据项就结束查找 注意在每一层sstable中查找数据时，都是按序依次查找sstable的。0层的文件比较特殊。由于0层的文件中可能存在key重合的情况，因此在0层中，文件编号大的sstable优先查找。理由是文件编号较大的sstable中存储的总是最新的数据。非0层文件，一层中所有文件之间的key不重合，因此可以借助sstable的元数据（一个文件中最小与最大的key值）进行快速定位，每一层只需要查找一个sstable文件的内容。 在memory db或者sstable的查找过程中，需要根据指定的序列号拼接一个internalKey，查找用户key一致，且seq号不大于指定seq 的数据. Compaction(压缩)Minor Compaction是指将内存中 Immutable memtable 的数据持久化为 0 层的 sstable 文件的过程，若干个0层文件中key是可能存在 overlap 的。是一个时效性非常高的操作，要求在尽可能短的时间内完成，否则会阻塞正常的写入操作。minor compaction 的优先级高于 major compaction，当同时发生时，会暂停 major compaction。 minor compaction 操作重写文件会带来很大的带宽压力以及短时间IO压力。 因此可以认为，它就是使用短时间的IO消耗以及带宽消耗换取后续查询的低延迟。 Major Compaction对相邻两个层的 sstable 进行多路归并排序，对文件中的 key 重新排序，过滤掉冗余版本后（视情况决定是否删除原文件），重新生成新的 sstable 的过程。 一次读取需要在内存中进行效率为O（log n）的查询。 若没有在内存中命中，则需要从 sstable 文件中查找。因为0层可能存在overlap，最差情况需要遍历0层所有文件。随着运行时间的增长，minor compaction 导致 0 层文件的个数会越来越多，查询效率也会越来越低，这显然是不能接受的。 因此，需要 major compaction 机制通过将0层中的文件，合并为若干个没有数据重叠的1层文件，一次的查找过程就可以进行优化。 compaction 归并的一个原因就是为了提高读取的效率，优化读放大问题。 一般 major compaction 的触发机制需要满足这几个条件： 当0层文件数超过预定的上限（默认为4个） 当 i 层文件的总大小超过(10 ^ i) MB 当某个文件无效读取的次数过多 因为 major compaction 是为了解决 0层文件过多导致读取效率低，但是如果仅关注 i 层的文件个数，在多次major compaction后，i+1 层会发生同样的问题，因此需要对每层的文件总数设定阈值。在某层文件个数达到阈值时，启动major compaction，提升读取效率，并且降低后续 compaction 的IO开销。 上述的机制可以保证合并的进行，但仍存在一种极端问题：当i层合并完成之后，i+1层的文件同时达到数据上限，更糟糕的是，最差情况下0-&gt;n层都会发生连锁更新。因此在合并时增加了这样两种机制： 错峰合并 一个文件一个查询的开销为10ms，若某个文件的查询次数过多，且查询在该文件中不命中，那么这种行为就可以视为无效查询开销 一个1MB的文件，其合并的开销为25ms。因此当一个1MB文件的文件无效查询超过25次时，对其合并 采样探测在sstable文件的metadata中，有一个额外的字段seekLeft，默认为文件的大小除以16kb。采样的过程：记录本次访问的第一个sstable文件，如果在该文件中访问命中，则不做任何处理；若未命中，对该文件的seekLeft标志做减一操作。seekLeft标志减少到0时，触发对该文件的错峰合并。 最终目标Compaction 的设计一方面需要保证Compaction的基本效果，另一方面又不会带来严重的IO压力。然而，并没有一种设计策略能够适用于所有应用场景或所有数据集。Compaction选择什么样的策略需要根据不同的业务场景、不同数据集特征进行确定。设计Compaction策略需要根据业务数据特点，目标就是降低读、写、空间三者的放大，不断权衡如下几点： 合理控制读放大：避免因Minor Freeze增多导致读取时延出现明显增大，避免请求读取过多SSTable； 合理控制写放大：避免一次又一次地Compact相同的数据； 合理控制空间放大：避免让不需要的多版本数据，已经删除的数据和过期的数据长时间占据存储空间，避免在Compaction过程中占用过多临时存储空间，及时释放已经Compact完成的无用SSTable的存储空间； Compaction流程压缩整体过程分为这几步： 寻找合适的输入文件 扩大输入文件集合 多路合并 寻找输入文件 对于 0 层文件数过多引发的合并场景或由于 i 层文件总量过大的合并场景，采用轮转的方法选择起始输入文件，记录了上一次该层合并的文件的最大key，下一次则选择在此key之后的首个文件。 对于错峰合并，起始输入文件则为该查询次数过多的文件。 扩大输入文件集合 在 i 层确定起始输入文件。 在 i 层中，查找与起始输入文件有key重叠的文件（这种情况一般发生在0层），构成 i 层的输入文件，结果为红线标注的文件。 利用 i 层的输入文件，在 i+1 层寻找有 key 重叠的文件，构成 i、i+1层的输入文件，结果为绿线标注的文件。 利用两层的输入文件，在不扩大 i+1 层输入文件的前提下，查找 i 层有key重叠的文件，构成最终全部的输入文件，结果为蓝线标注的文件。 多路合并将输入文件内所有的数据项，按序排列之后，输出到 i+1 层的若干个新文件中。在合并的过程中，相同key的冗余数据，仅保留最新版本的那一份。如果文件中存在某些仍在使用的旧版本数据，此时不能立即删除，需要等到使用结束，释放文件句柄后，根据引用计数来清除。","categories":[],"tags":[]},{"title":"sync.Pool原理解析","slug":"sync-Pool原理解析","date":"2021-05-23T10:34:06.000Z","updated":"2021-11-03T04:04:31.554Z","comments":true,"path":"2021/05/23/sync-Pool原理解析/","link":"","permalink":"http://8090lambert.cn/2021/05/23/sync-Pool原理解析/","excerpt":"","text":"sync.Pool 介绍拥有垃圾回收特性的语言里，gc发生时都会带来性能损耗，为了减少gc影响，通常的做法是减少小块对象内存频繁申请，让每次发生垃圾回收时scan和clean活跃对象尽可能的少。sync.Pool可以帮助在程序构建了对象池，提供对象可复用能力，本身是可伸缩且并发安全的。 主要结构体Pool对外导出两个方法： Get 和 Put，Get是用来从Pool中获取可用对象，如果可用对象为空，则会通过New预定义的func创建新对象。Put是将对象放入Pool中，提供下次获取。 Getfunc (p *Pool) Get() interface{} { if race.Enabled { race.Disable() } l, pid := p.pin() x := l.private l.private = nil if x == nil { // Try to pop the head of the local shard. We prefer // the head over the tail for temporal locality of // reuse. x, _ = l.shared.popHead() if x == nil { x = p.getSlow(pid) } } runtime_procUnpin() if race.Enabled { race.Enable() if x != nil { race.Acquire(poolRaceAddr(x)) } } if x == nil &amp;&amp; p.New != nil { x = p.New() } return x } 首先看下GET方法的逻辑（在看前需要对gmp调度模型有大致了解） 通过pin拿到poolLocal和当前 goroutine 绑定运行的P的 id。每个goroutine创建后会挂在P结构体上；运行时，需要绑定P才能在M上执行。因此，对private指向的poolLocal操作无需加锁，都是线程安全的 设置x，并且清空private x为空说明本地对象未设置，由于P上存在多个G，如果一个时间片内协程1把私有对象获取后置空，下一时间片g2再去获取就是nil。此时需要去share中获取头部元素，share是在多个P间共享的，读写都需要加锁，但是这里并未加锁，具体原因等下讲 如果share中也返回空，调用getSlow()函数获取，等下具体看内部实现 runtime_procUnpin()方法，稍后我们详细看 最后如果还是未找到可复用的对象, 并且设置了New的func，初始化一个新对象 Pool的local字段表示poolLocal指针。获取时，优先检查private域是否为空，为空时再从share中读取，还是空的话从其他P中窃取一个，类似goroutine的调度机制。 pin刚才的几个问题，我们具体看下。首先，pin方法获取当前P的poolLocal,方法逻辑比较简单 func (p *Pool) pin() *poolLocal { pid := runtime_procPin() s := atomic.LoadUintptr(&amp;p.localSize) // load-acquire l := p.local // load-consume if uintptr(pid) &lt; s { return indexLocal(l, pid) } return p.pinSlow() } runtime_procPin返回了当前的pid，实现细节看看runtime内部 //go:linkname sync_runtime_procPin sync.runtime_procPin //go:nosplit func sync_runtime_procPin() int { return procPin() } //go:linkname sync_runtime_procUnpin sync.runtime_procUnpin //go:nosplit func sync_runtime_procUnpin() { procUnpin() } //go:nosplit func procPin() int { _g_ := getg() mp := _g_.m mp.locks++ return int(mp.p.ptr().id) } //go:nosplit func procUnpin() { _g_ := getg() _g_.m.locks-- } pin获取当前goroutine的地址，让g对应的m结构体中locks字段++，返回p的id。unPin则是对m的locks字段–，为什么要这么做？ 协程发生调度的时机之一：如果某个g长时间占用cpu资源，便会发生抢占式调度，可以抢占的依据就是locks == 0。其实本质是为了禁止发生抢占。 // One round of scheduler: find a runnable goroutine and execute it. // Never returns. func schedule() { _g_ := getg() //调度时，会判断`locks`是否为0。 if _g_.m.locks != 0 { throw(&quot;schedule: holding locks&quot;) } ... } 为什么要禁止调度呢?因为调度是把m和p的绑定关系解除，让p去绑定其他线程，执行其他线程的代码段。在get时，首先是获取当前goroutine绑定的p的private，不禁止调度的话，后面的获取都不是当前协程的运行时的p，会污染其他p上的数据，引起未知错误。 poolChainpoolChain是一个双端链表，结构体如下： type poolChain struct { head *poolChainElt tail *poolChainElt } poolChain.popHeadpoolChain.popHead获取时，首先从poolDequeue的popHead方法获取，未获取到时，找到prev节点，继续重复查找，直到返回nil。 func (c *poolChain) popHead() (interface{}, bool) { d := c.head for d != nil { if val, ok := d.popHead(); ok { return val, ok } // There may still be unconsumed elements in the // previous dequeue, so try backing up. d = loadPoolChainElt(&amp;d.prev) } return nil, false } 这里注意区分poolChain和poolDequeue，两个结构存在同名的方法，但是结构和逻辑完全不同 type poolChain struct { // head is the poolDequeue to push to. This is only accessed // by the producer, so doesn&#39;t need to be synchronized. head *poolChainElt // tail is the poolDequeue to popTail from. This is accessed // by consumers, so reads and writes must be atomic. tail *poolChainElt } type poolChainElt struct { poolDequeue next, prev *poolChainElt } type poolDequeue struct { headTail uint64 vals []eface } 需要说明下：poolChainElt组成的链表结构和我们常见的链表方向相反，从head -&gt; tail的方向是prev，反之是next;poolDequeue 是一个环形链表，headTail字段保存首尾地址，其中高32位表示head，低32位表示tail. poolDequeue.popHeadfunc (d *poolDequeue) popHead() (interface{}, bool) { var slot *eface for { ptrs := atomic.LoadUint64(&amp;d.headTail) head, tail := d.unpack(ptrs) if tail == head { return nil, false } head-- ptrs2 := d.pack(head, tail) if atomic.CompareAndSwapUint64(&amp;d.headTail, ptrs, ptrs2) { slot = &amp;d.vals[head&amp;uint32(len(d.vals)-1)] break } } val := *(*interface{})(unsafe.Pointer(slot)) if val == dequeueNil(nil) { val = nil } *slot = eface{} return val, true } 看到if tail == head，如果首位地址相同说明链表整体为空，证明poolDequeue确实是环形链表； head--后pack(head, tail)得到新的地址ptrs2，如果ptrs == ptrs2，修改headTail地址； 把slot转成interface{}类型的value； getSlow如果从shared的popHead中没拿到可服用的对象，需要通过getSlow来获取 func (p *Pool) getSlow(pid int) interface{} { size := atomic.LoadUintptr(&amp;p.localSize) // load-acquire locals := p.local // load-consume // 遍历locals，从其他P上的尾部窃取 for i := 0; i &lt; int(size); i++ { l := indexLocal(locals, (pid+i+1)%int(size)) if x, _ := l.shared.popTail(); x != nil { return x } } size = atomic.LoadUintptr(&amp;p.victimSize) if uintptr(pid) &gt;= size { return nil } // 尝试从victim指向的poolLocal中，按照先private -&gt; shared的顺序获取 locals = p.victim l := indexLocal(locals, pid) if x := l.private; x != nil { l.private = nil return x } for i := 0; i &lt; int(size); i++ { l := indexLocal(locals, (pid+i)%int(size)) if x, _ := l.shared.popTail(); x != nil { return x } } atomic.StoreUintptr(&amp;p.victimSize, 0) return nil } 通过遍历locals获取对象，使用到victim字段指向的[]poolLocal。这里其实引用了一种叫做Victim Cache的机制，具体解释详见这里。 poolChain.popTailfunc (c *poolChain) popTail() (interface{}, bool) { d := loadPoolChainElt(&amp;c.tail) if d == nil { return nil, false } for { d2 := loadPoolChainElt(&amp;d.next) if val, ok := d.popTail(); ok { return val, ok } if d2 == nil { return nil, false } if atomic.CompareAndSwapPointer((*unsafe.Pointer)(unsafe.Pointer(&amp;c.tail)), unsafe.Pointer(d), unsafe.Pointer(d2)) { storePoolChainElt(&amp;d2.prev, nil) } d = d2 } } d2是d的next节点，d已经为链表尾部了，这里也应证了我们刚才说到的poolChain链表的首尾方向和正常的链表是相反的（至于为啥要这么设计，我也是比较懵逼）。如果d2为空证明已经到了链表的头部，所以直接返回； 从尾部节点get成功时直接返回，已经返回的这个位置，等待着下次get遍历时再删除。由于是从其他的P上窃取，可能发生同时多个协程获取对象，需要保证并发安全； 为什么popHead不去删除链表节点，两个原因吧。第一个，popHead只有当前协程在自己的P上操作，popTail是窃取，如果在popHead中操作，也需要原子操作，作者应该是希望把get阶段的开销降到最低；第二个，因为poolChain结构本身是链表，无论在哪一步做结果都是一样，不如统一放在尾部获取时删除。 poolDequeue.popTailfunc (d *poolDequeue) popTail() (interface{}, bool) { var slot *eface for { ptrs := atomic.LoadUint64(&amp;d.headTail) head, tail := d.unpack(ptrs) if tail == head { return nil, false } ptrs2 := d.pack(head, tail+1) if atomic.CompareAndSwapUint64(&amp;d.headTail, ptrs, ptrs2) { slot = &amp;d.vals[tail&amp;uint32(len(d.vals)-1)] break } } val := *(*interface{})(unsafe.Pointer(slot)) if val == dequeueNil(nil) { val = nil } slot.val = nil atomic.StorePointer(&amp;slot.typ, nil) return val, true } 和poolDequeue.popHead方法逻辑基本差不多，由于popTail存在多个协程同时遍历，需要通过CAS获取，最后设置slot为空。 Putfunc (p *Pool) Put(x interface{}) { if x == nil { return } if race.Enabled { if fastrand()%4 == 0 { // Randomly drop x on floor. return } race.ReleaseMerge(poolRaceAddr(x)) race.Disable() } l, _ := p.pin() if l.private == nil { l.private = x x = nil } if x != nil { l.shared.pushHead(x) } runtime_procUnpin() if race.Enabled { race.Enable() } } put方法相关逻辑和get很像，先设置poolLocal的private，如果private已有，通过shared.pushHead写入。 poolChain.pushHeadfunc (c *poolChain) pushHead(val interface{}) { d := c.head if d == nil { // 初始化环，数量为2的幂 const initSize = 8 d = new(poolChainElt) d.vals = make([]eface, initSize) c.head = d storePoolChainElt(&amp;c.tail, d) } if d.pushHead(val) { return } // 如果环已满，按照2倍大小创建新的ring。注意这里有最大数量限制 newSize := len(d.vals) * 2 if newSize &gt;= dequeueLimit { // Can&#39;t make it any bigger. newSize = dequeueLimit } d2 := &amp;poolChainElt{prev: d} d2.vals = make([]eface, newSize) c.head = d2 storePoolChainElt(&amp;d.next, d2) d2.pushHead(val) } 如果节点是空，则创建一个新的poolChainElt对象作为头节点,然后调用pushHead放入到环状队列中.如果放置失败，那么创建一个2倍大小且不超过dequeueLimit（2的30次方）的poolChainElt节点。所有的vals长度必须为2的整数幂。 func (d *poolDequeue) pushHead(val interface{}) bool { ptrs := atomic.LoadUint64(&amp;d.headTail) head, tail := d.unpack(ptrs) if (tail+uint32(len(d.vals)))&amp;(1&lt;&lt;dequeueBits-1) == head { return false } slot := &amp;d.vals[head&amp;uint32(len(d.vals)-1)] typ := atomic.LoadPointer(&amp;slot.typ) if typ != nil { return false } if val == nil { val = dequeueNil(nil) } *(*interface{})(unsafe.Pointer(slot)) = val atomic.AddUint64(&amp;d.headTail, 1&lt;&lt;dequeueBits) return true } 首先判断ring是否大小已满，然后找到head位置对应的slot判断typ是否为空，因为popTail是先设置 val，再将 typ 设置为 nil，有冲突会直接返回。 结论：整个对象池通过几个主要的结构体构成，它们之间关系如下： poolCleanup注册了全局清理的func，在每次gc开始时运行。既然每次gc都会清理pool内对象，那么对象复用的优势在哪里呢？poolCleanup在每次gc时，会将allPools里的对象写入oldPools对象后再清除自身对象。那么就是说，如果申请的对象，会经过两次gc后，才会被彻底回收。p.local会先设置为p.victim，是不是有点类似新生代、老生代的感觉。 func init() { runtime_registerPoolCleanup(poolCleanup) } func poolCleanup() { for _, p := range oldPools { p.victim = nil p.victimSize = 0 } // Move primary cache to victim cache. for _, p := range allPools { p.victim = p.local p.victimSize = p.localSize p.local = nil p.localSize = 0 } oldPools, allPools = allPools, nil } 可以看出，在gc发生不频繁的场景，sync.Pool对象复用就可以减少内存的频繁申请和回收。 References https://mp.weixin.qq.com/s?__biz=MzA4ODg0NDkzOA==&amp;mid=2247487149&amp;idx=1&amp;sn=f38f2d72fd7112e19e97d5a2cd304430&amp;source=41#wechat_redirect https://medium.com/@genchilu/whats-false-sharing-and-how-to-solve-it-using-golang-as-example-ef978a305e10","categories":[],"tags":[{"name":"go","slug":"go","permalink":"http://8090lambert.cn/tags/go/"}]},{"title":"背八股 要谨慎","slug":"背八股 要谨慎","date":"2021-01-04T10:11:47.000Z","updated":"2021-11-04T12:36:52.561Z","comments":true,"path":"2021/01/04/背八股 要谨慎/","link":"","permalink":"http://8090lambert.cn/2021/01/04/背八股 要谨慎/","excerpt":"","text":"三天的元旦节，导致大量的面试积压，从早上来公司就开始疯狂补作业。自身本来就对面试没有太多激情，比较排斥八股文，刷题怪的这种套路。临近下班前，和一位候选人聊到了他的项目（本人比较乐意从项目的设计来考察技术的深度和广度）。 项目的背景大致是教育直播相关的，对于直播的一些必要且时效性不高的数据，通过Redis做主动缓存，应对课堂上大流量的请求。一般这种回答，我都愿意继续问一问，看看有没有更加全面的考虑和一些必要的兜底预案，算是讨论交流。这位同学回答说，他们线上的 Redis 使用读写分离，采用1主多从的架构部署。 按照经验来看，redis 单实例应对 10W 以内的读写qps，没有什么特别压力。但是，存储组件在应对主从分离时，面临最大的是数据一致性。所以，顺口问了候选人有没有在项目中碰到过因为读写分离导致的一致性问题，候选人谈到因为redis的过期key，导致在读取时会读到过期的key。 我突然有些吃惊。根据自己的知识面和记忆: Redis在主从架构下，如果当前key已经过期， 在通过master获取过期key时，会执行异步懒删除，并将删除指令传播到slave节点，同时返回给clint不存在； 如果通过slave获取过期key时，由于主从同步之间网络耗时一定存在，会通过一个logical clock来返回key不存在 持着怀疑的态度向他确认了为什么会读到过期的key。在追问下，他仅告诉我在主节点读取时会将删除命令同步到从节点，而从节点的逻辑却没有提到，而且信心满满的告诉我他昨天刚看完书，肯定是这样的……… 这一段摘自Redis官方文档中的描述，解释了Redis在处理复制时对过期key的处理策略。其中第一条，说明了slave节点删除key的指令是来由master节点同步而来。第二条便是说，在slave节点上通过 逻辑时钟 来提示key不存在，这个操作是不违反数据一致性的读取，因为master节点的del命令会到达。 在github中也可以找到，redis作者在这个 commit 中就修复了这一问题。 robj *lookupKeyReadWithFlags(redisDb *db, robj *key, int flags) { robj *val; if (expireIfNeeded(db,key) == 1) { /* If we are in the context of a master, expireIfNeeded() returns 1 * when the key is no longer valid, so we can return NULL ASAP. */ if (server.masterhost == NULL) goto keymiss; /* However if we are in the context of a slave, expireIfNeeded() will * not really try to expire the key, it only returns information * about the &quot;logical&quot; status of the key: key expiring is up to the * master in order to have a consistent view of master&#39;s data set. * * However, if the command caller is not the master, and as additional * safety measure, the command invoked is a read-only command, we can * safely return NULL here, and provide a more consistent behavior * to clients accessing expired values in a read-only fashion, that * will say the key as non existing. * * Notably this covers GETs when slaves are used to scale reads. */ if (server.current_client &amp;&amp; server.current_client != server.master &amp;&amp; server.current_client-&gt;cmd &amp;&amp; server.current_client-&gt;cmd-&gt;flags &amp; CMD_READONLY) { goto keymiss; } } val = lookupKey(db,key,flags); if (val == NULL) goto keymiss; server.stat_keyspace_hits++; return val; keymiss: if (!(flags &amp; LOOKUP_NONOTIFY)) { notifyKeyspaceEvent(NOTIFY_KEY_MISS, &quot;keymiss&quot;, key, db-&gt;id); } server.stat_keyspace_misses++; return NULL; } 在源码中可以看到，如果是过期 key 并且 当前实例不是master，直接返回null 最后，问一个开放性的问题，为什么这里没有直接删除呢？因为系统时钟是不可靠的，如果slave的时钟比master快，key可能会在master之前过期，这时client在slave上读取并删除了实例上的key, 后续master可能会发送一个命令来设置key的新过期时间，但由于key在slave上过期而失败。作者这个修改还是依赖master的命令来对slave上的key做写操作，只是对slave上存在读请求时发生的不一致行为做了兼容。 所以，大家在学习”八股文”的时候，还是要结合代码或者官方文档推敲细节哟，哈哈哈~","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://8090lambert.cn/tags/redis/"}]},{"title":"一致性协议raft、zab的区别","slug":"一致性协议raft、zab的区别","date":"2020-11-03T13:46:43.000Z","updated":"2021-11-03T09:20:47.406Z","comments":true,"path":"2020/11/03/一致性协议raft、zab的区别/","link":"","permalink":"http://8090lambert.cn/2020/11/03/一致性协议raft、zab的区别/","excerpt":"","text":"目的为解决大型分布式系统中多个副本日志及状态机一致性问题。 Raft介绍协议由三部分组成：Leader election、Log Replication、Safety。分别对应着三种状态的身份：leader(工作时的首要进程)、candidate(选举时可能会成为leader)、follower(副本). Raft选举选举的时机一般有两种： 初始化时，系统中不存在 Leader 角色。所有节点均为 candidate 角色 follower 在 election timeout 后没有收到 leader心跳，follower 变为 candidate 角色 candidate向其他节点发起 leader election 请求，得到超过半数其他 candidate 的回复后成为leader. 同时选举的过程存在极端情况： 所有选票被多个节点瓜分，没有选出leader。 为避免发生多个 candidate 同时发生选举，而导致的多次选举。raft采用 random election timeout 机制，使每个节点的超时时间不同。一般来说需要保证 election timeout &gt; broadcast 的间隔时间，否则，election timeout 无法说明与leader断开连接. 安全性两个限制条件保证日志复制的安全限制一：选举阶段节点 m 向节点 n 发送了 RequestVote RPC，如果节点 n 发现节点 m 的数据没有自己新，则节点 n 拒绝节点 m 的投票请求。这里的“新”包含两个方面，term 更大的数据更新，term 相同，index更大的数据更新。 选举时，只有日志最新的candidate才有机会得到超过半数的投票。其中有这两个条件: term越大的节点越容易成为leader term相同的情况下，index越大的节点越容易成为leader 限制二：不直接提交之前 term 的log，必须通过提交本 term 的 log，间接的提交之前 term 的 log。选举时，在条件1的情况下，term不同，可能选举出来的leader不包含所有过半提交的日志。要解决这个问题，需要在 那么就需要对过半提交通过限制二来约束。一旦加上了这个限制意味着含有所有过半提交的日志的几个节点必然拥有目前最大的term，这样的话再次进行leader选举时，就避免了条件1的误判。 Raft 对比 ZabLeader election 阶段 Issue Raft Zab Leader检测 通过Follower心跳 leader: 维护Quorum集合； follower: 通过长链接 过期leader判断 term_id epoch leader投票过程 每个节点每轮只投一次票 每次选举节点可能产生多次投票，选出最新的 leader 可用性检查：Raft 协议 leader 宕机仅仅由 folower 进行检测，当 folower 收不到 leader 心跳时，则认为 leader 宕机，变为 candidate。Zk 的 leader down 机分别由 leader 和 folower 检测，leader 维护了一个 Quorum 集合，当该 Quorum 集合不再超过半数，leader 自动变为 LOOKING 状态。folower 与 leader 之间维护了一个超链接，连接断开则 folower 变为 LOOKING 状态。 过期 leader 的屏蔽：Raft 通过 term 识别过期的 leader。Zk 通过 Epoch识别过期的 leader。这点两者是相似的。 leader 选举的投票过程：Raft 每个选举周期每个节点只能投一次票，选举失败进入下次周期才能重新投票。Zk 每次选举节点需要不断的变换选票以便选出数据最新的节点为 leader。 保证 commited 的数据出现在未来 leader 中：Raft选取 leader 时拒绝数据比自己旧的节点的投票。Zk 通过在选取 leader 时不断更新选票使得拥有最新数据的节点当选 leader。 Zab在leader选举出来后会立即进行数据同步过程，同步下最新的epoch（相当于term），数据同步之后就不会存在Raft（经过几个term变更后）日志错乱的现象。Zab是正式对外提交服务之前先清理好数据的不一致问题，Raft是延迟清理的，在后续的复制过程中不断的进行纠正更新。 Log Replication 阶段 Issue Raft Zab 新leader数据同步 通过AppendEntry Rpc每次同步 通过 Recovery Phase 同步 新leader数据同步量 增量同步 增量或全量同步 之前未commit数据处理 commit 当前term时，会将未提交数据一起commit Recovery Phase阶段commit 新加入集群节点数据同步 不阻塞写请求 Recovery Phase 会阻塞写请求 选取新 leader 后数据同步：Raft 没有固定在某个特定的阶段做这件事情，通过每个节点的 AppendEntry RPC 分别做数据同步。Zk 则在新leader 选举之后，有一个 Recovery Phase 做这个件事情。 选取新 leader 后同步的数据量：Raft 只需要传输和新 leader 差异的部分。Zab 的原始协议需要传输 leader 的全部数据，Zk 优化后，视情况而定，最坏情况下需要传输 leader 全部数据。 新 leader 对之前 leader 未 commit 数据的处理：Raft 不会直接 commit 之前 leader 的数据，通过 commit 本 term 的数据间接的 commit 之前 leader 的数据。Zk 在 Recovery Phase直接 commit 之前 leader 的数据。 新加入集群节点的数据同步：Raft 对于新加入集群的节点数据同步不会影响客户端的写请求。Zk 对于新加入集群的节点，需要单独走一下 Recovery Phase，目前是通过读写锁同步的，因此会阻塞客户端的写请求。（Zk 可以在这里使用 copy-on-write 机制避免阻塞问题？？） 脑裂问题 Raft Zab 通过增加region leader角色 + lease机制，每个请求通过region leader转发至raft leader 通过Quorum过半机制避免 Tips： Etcd 不存在脑裂 Reference https://niceaz.com/2018/11/03/raft-and-zab/https://www.jianshu.com/p/072380e12657","categories":[],"tags":[]},{"title":"IO Optimize","slug":"IO Optimize","date":"2020-07-25T03:15:05.000Z","updated":"2021-11-03T06:41:14.370Z","comments":true,"path":"2020/07/25/IO Optimize/","link":"","permalink":"http://8090lambert.cn/2020/07/25/IO Optimize/","excerpt":"","text":"一、传统IO传输传统的数据传输，不论是文件，还是写文件，都是会经过从 磁盘-》内核缓冲区 -》用户应用缓冲区 -》Socket 缓冲区 -》 网卡，这些步骤。这其中，会涉及到 4次用户态到内核态的上下文切换、4次用户态和内核态之间的数据拷贝。 以读文件为例：线程在用户空间发起read()读文件，线程从用户态切换为内核态DMA将磁盘数据拷贝到内核缓存后，CPU又将数据从内核缓存拷贝至用户缓存，这时线程又从内核态切换为用户态CPU将数据从用户缓存拷贝至socket缓存，线程又从用户态切换到内核态DMA将数据从内核缓存拷贝到网卡，read()调用结束返回，线程又从内核态切换到用户态 方式一：mmap + write实现零拷贝mmap，Memory Mapped Files：简称 mmap，也有叫 MMFile 的，使用 mmap 的目的是将内核中读缓冲区（read buffer）的地址与用户空间的缓冲区（user buffer）进行映射。从而实现内核缓冲区与应用程序内存的共享，省去了将数据从内核读缓冲区（read buffer）拷贝到用户缓冲区（user buffer）的过程。它的工作原理是直接利用操作系统的 Page 来实现文件到物理内存的直接映射。完成映射之后你对物理内存的操作会被同步到硬盘上。使用这种方式可以获取很大的 I/O 提升，省去了用户空间到内核空间复制的开销，在用户态对映射区域的写操作数据会同时在内核态缓存中存在用户空间发起mmap系统调用DMA将数据从文件系统拷贝到内核缓存中，并和用户空间堆外缓存建立映射关系CPU从用户缓存读取到数据后，将数据拷贝到socket缓存中，线程从用户态切换到内核态DMA将数据从socket缓存拷贝到网卡，调用write()结束后返回，线程从内核态切换到用户态整个过程发生了3次拷贝，2次DMA，1次CPU；4次线程切换 方式二：sendfilesendfile，在Linux内核2.1版本之后，提供了sendfile()函数sendfile(int out_fd, int in_fd, off_t *offset, size_t count); // out_fd 参数代表待写入的文件描述符// in_fd 参数代表待读取的文件描述符// *offset 参数代表指定从读取文件流的哪个位置开始读，为空时则使用读入文件流默认的起始位置// count 参数代表传输的字节数sendfile函数实现了内核态和用户态之间的“零拷贝”：就是将数据的拷贝全部控制在内核态中，省去传统读取文件方式中 2次上下文切换 和 2次数据拷贝（伴随着上下文切换时，发生在内核态到用户态之间的拷贝），具体步骤如下：用户空间发起sendfile()函数调用，设置读取数据和写入输入的文件描述符、读取字节的偏移量、读取的字节长度，进程从用户态切换到内核态DMA 把磁盘的数据拷贝到内核缓存(Page Cache)中CPU 从内核缓存中，将数据拷贝至socket缓存中DMA 将socket缓存中的数据拷贝至网卡，sendfile调用完成，进程由内核态切换至用户态 整个过程发生了 2次上下文切换 和 3次数据拷贝，和上文说的貌似不一致。没错，在 sendfile()的 man page 中有这样的定义：In Linux kernels before 2.6.33, out_fd must refer to a socket. Since Linux 2.6.33 it can be any file. 因此，在Linux 2.6.33 之前的内核版本，写入的 fd 必须为 socket，因此数据在内核缓冲区后，需要再次拷贝到socket缓冲区，而在 2.6.33 版本之后的实现略有不同 如图，当数据被拷贝至内核缓冲区时，通过DMA Gather控制器，直接拷贝至网卡。因为全程没有cpu参与数据的搬运，所有的数据都是通过 DMA 来进行传输的，实现了真正意义上的“零拷贝”。 Referencehttps://www.cnblogs.com/xiaolincoding/p/13719610.htmlhttps://www.cnblogs.com/ericli-ericli/articles/12923420.html","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"http://8090lambert.cn/tags/linux/"}]},{"title":"Kafka四问","slug":"Kafka四问","date":"2020-01-27T14:35:09.000Z","updated":"2021-11-03T06:41:03.385Z","comments":true,"path":"2020/01/27/Kafka四问/","link":"","permalink":"http://8090lambert.cn/2020/01/27/Kafka四问/","excerpt":"","text":"基本概念HW（HighWater）高水位，表示已经提交（commit）的最大日志偏移量（offset）,已提交是指 ISRs 中所有节点都已同步到这条日志 LEO（Log endOffset）日志最后偏移量，表示日志中下一条待写入消息的Offset ISRs每个 partition 有一个 leader 和多个 follower（副本因子&gt;1）,其中日志状态保持和 leader 同步的 follower 集合被称为 ISRs，于其对应的有落后于 leader 日志状态的副本集合称为 OSRs，ISRs + OSRs = AR（All Replication） 一、为什么不使用Quorum策略？Quorum 机制Quorum 机制对集群节点数量有要求，如果需要容忍N个节点故障，集群整体需要2N+1个节点；Quorum 主要能应对集群出现 脑裂问题 ISRs 机制ISR 机制，如果要容忍N个节点故障，只需保证 ISR 中存在N+1个节点；ISR通过zookeeper来避免来保证leader的唯一 二、数据一致性如何保证？ 关闭unclean，保证新的Leader副本，是从ISRs集合中进行选举 设置ISRs的最小副本数 &gt;= 2（ISR中包括Leader副本） 以下两类特殊的数据一致性问题，通过引入Leader Epoch机制解决（类似Raft的Terms） 日志丢失问题 某一个时刻，Leader A收到 producer 的消息m2，并成功写入本地Page Cache中。Follower B拉取到m2，写入Page Cache但未刷盘。同步给Leader A HW=2，Leader A更新自身HW=2。此时，A、B同时崩溃 B先恢复，并成为leader 节点，A 恢复后成为follower，HW=2所以不需要截断自身的日志。 Leader B 收到了 producer 的m3消息，此时offset=1的日志，两个节点出现了不一致现象。（A=m2, B=m3） 引入Leader Epoch解决两个问题产生的原因，都是follower节点在异常情况恢复后，通过自身的HW来决定日志的状态。其实，上面分析的场景和raft中的日志恢复类似，raft中的follower是和leader的日志不一致时，会以leader的日志为准进行日志恢复。而raft中的日志恢复很重要的一点是follower根据leader任期号进行日志比对，快速进行日志恢复，follower需要判断新旧leader的日志（可能由于分区等问题，短暂出现2个leader的情况），以最新leader的数据为准。这里的leader epoch和raft中的 term 类似，用一个严格单调递增的id来标志。follower每次奔溃重启后，都需要去leader那边确认下当前leader的日志是从哪个offset开始的。 介绍这几个概念各自的用处： Leader Epoch：Leader纪元，单调递增的int值。 Leader Epoch Start Offset：Leader的第一个日志偏移，也标志了旧Leader最后一条日志的偏移 Leader Epoch Request：Follower向Leader发送请求时，Leader会判断当前纪元是否是自身的epoch，如果是则返回自己的LEO，否则返回下一个纪元的Leader Epoch Start Offset，follower通过这个值做日志的截断处理 日志截断仅会发生在 follower 节点上 三、如何保证高性能？ 多副本：多副本机制，带来多节点的并发能力。由于内部的优先副本分配策略，可以尽可能的保证做到Leader副本的负载均衡（Leader副本被均匀的分配在不同的broker节点）。 磁盘的顺序读写：由于topic的每个partition的消息是不可变的，新消息写入时，不断追加到partition日志的末尾。因此，可以利用磁盘顺序写的特点。数据的删除，因为每个log 被划分为多个segment，每个segment对应一个物理文件，通过删除文件的方式清理partition内的数据。 利用Page Cache：引入Page Cache，由OS来决定刷盘的时机。Page Cache的使用，可以将多个非连续、小块写操作合并，提高磁盘的写入效率，同时减少对文件系统的频繁调用。Kafka本身是Java语言栈，如果使用堆内存做Cache，在kafka进程重启时，数据会丢失，而且堆内内存的占用比数据本身占用的内存大（因为有结构、辅助字段等必要信息存储）。 零拷贝技术：生产者写入数据时，通过mmap提升写数据消息日志落盘的性能；消费者读取消息时。 批量处理：在很多情况下，系统的瓶颈不是CPU或磁盘，而是网络IO传输。kafka的producer在收到消息时，在积累足够多的消息或等待足够长的时间后，再发送到broker，批处理分摊了网络传输开销，提升带宽利用率，类似 TCP 的 Nagle 算法。 数据压缩：合并消息，减少数据体积 四、如何保证消息的幂等性和顺序性？幂等性一般消息系统需要具备三种常见的语义，at most once(至多一次)，at least once(至少一次)，exactly once(恰好一次)，大多数系统都可以做到at most once 和 at least once。 PID (producer ID)，用来表示每个 producer 的唯一性（在开启幂等时，每次发送给 broker 时消息中都携带） sequence numbers，producer 发送给 broker 的每条消息都会带响应的 sequence number，逐次递增在配置 enable.idempotence = true 时(开启幂等配置)，通过 pid + sequence number，保证了同一个 producer 在 topic-partition 维度的幂等性 顺序性kafka producer 采用异步发送机制。KafkaProducer.send(ProducerRecord) 方法仅仅是把这条消息放入一个缓存中(即RecordAccumulator，本质上使用了队列来缓存记录)，同时后台的IO线程会不断扫描该缓存区，将满足条件的消息封装到某个 batch 中然后发送出去。在这个过程中有一个数据丢失的窗口：若IO线程发送之前 producer 挂掉了，累积在 Accumulator 中的数据的确有可能会丢失。而且当设置 max.in.flight.requests.per.connection &gt; 1 并且 retries &gt;= 1 时，发送到同一个 topic-partition 中的消息中，可能由于网络等其他问题，导致实际顺序与写入顺序不一致，max.in.flight.requests.per.connection = 1 保证有序。在开启了幂等的情况下，可以保证写入的顺序性 工作流程未设置幂等时 设置幂等时","categories":[],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://8090lambert.cn/tags/kafka/"}]},{"title":"Golang的interface探究","slug":"Golang的interface探究","date":"2019-10-11T10:15:49.000Z","updated":"2021-05-09T14:50:16.593Z","comments":true,"path":"2019/10/11/Golang的interface探究/","link":"","permalink":"http://8090lambert.cn/2019/10/11/Golang的interface探究/","excerpt":"","text":"golang被诟病最多的，没有泛型应该算一个。作为强类型语言来说，没有泛型很多时候在业务开发上会有些不适应，但是它有个interface类型，被很多人拿来当泛型玩，如果你了解它的原理也是没问题的。但是你真的了解吗？ Interfacegolang 中的interface，可以将任意类型的变量赋予它。常见的我们区分两种，一种就是struct类型的，因为struct可能会有func；另外一种，就是非结构体的普通类型（下面提到的普通类型，都是指代除struct外的类型） eface 1 package main 2 3 import &quot;fmt&quot; 4 5 func main() { 6 var x int 7 var y interface{} 8 x = 1 9 y = x 10 fmt.Println(y) 11 } 当我们把int类型的变量赋值给interface类型时，会发生什么： TEXT main.main(SB) /home/xiaoju/gomodule/runtime/main.go main.go:5 0x4a23a0 64488b0c25f8ffffff mov rcx, qword ptr fs:[0xfffffff8] main.go:5 0x4a23a9 488d4424f8 lea rax, ptr [rsp-0x8] main.go:5 0x4a23ae 483b4110 cmp rax, qword ptr [rcx+0x10] main.go:5 0x4a23b2 0f86c7000000 jbe 0x4a247f =&gt; main.go:5 0x4a23b8* 4881ec88000000 sub rsp, 0x88 main.go:5 0x4a23bf 4889ac2480000000 mov qword ptr [rsp+0x80], rbp main.go:5 0x4a23c7 488dac2480000000 lea rbp, ptr [rsp+0x80] main.go:6 0x4a23cf 48c744243000000000 mov qword ptr [rsp+0x30], 0x0 main.go:7 0x4a23d8 0f57c0 xorps xmm0, xmm0 main.go:7 0x4a23db 0f11442448 movups xmmword ptr [rsp+0x48], xmm0 main.go:8 0x4a23e0 48c744243001000000 mov qword ptr [rsp+0x30], 0x1 main.go:9 0x4a23e9 48c7042401000000 mov qword ptr [rsp], 0x1 main.go:9 0x4a23f1 e89a70f6ff call $runtime.convT64 追到runtime的convT64方法，一探究竟。 // type uint64InterfacePtr uint64 // var uint64Eface interface{} = uint64InterfacePtr(0) // var uint64Type *_type = (*eface)(unsafe.Pointer(&amp;uint64Eface))._type func convT64(val uint64) (x unsafe.Pointer) { if val == 0 { x = unsafe.Pointer(&amp;zeroVal[0]) } else { x = mallocgc(8, uint64Type, false) *(*uint64)(x) = val } return } 这个方法返回了 val 的指针，其中uint64Type就是一个 0 值的uint64指针。有个疑问，这里uint64Type定义时，eface 是什么： type eface struct { _type *_type data unsafe.Pointer } 这个结构体，恰好满足了，对于普通类型转换interface，或者说是将普通类型赋值给interface所必须的两个字段，当前类型的type和值（这里貌似有点绕口）。真实的是，eface确实就是表示这类interface的结构体，在runtime中，还能看到其他普通类型的转换，convTslice、convTstring、convT64、convT32等其他几个方法。 iface如果是一个拥有func的struct类型的变量，赋值给另一个interface，这类的interface在底层是怎么存的呢。如下所示： 1 package main 2 3 import &quot;fmt&quot; 4 5 type Human interface{ Introduce() string } 6 7 type Bob struct{ Human } 8 9 func (b Bob) Introduce() string { return &quot;Name: Bob&quot; } 10 11 func main() { 12 var y Human 13 x := Bob{} 14 y = x 15 fmt.Println(y) 16 } TEXT main.main(SB) /Users/such/gomodule/runtime/main.go main.go:11 0x10b71a0 65488b0c2530000000 mov rcx, qword ptr gs:[0x30] main.go:11 0x10b71a9 488d4424d0 lea rax, ptr [rsp-0x30] main.go:11 0x10b71ae 483b4110 cmp rax, qword ptr [rcx+0x10] main.go:11 0x10b71b2 0f860f010000 jbe 0x10b72c7 ...省略部分指令 main.go:14 0x10b7202 e84921f5ff call $runtime.convT2I 看汇编代码，在 16 行时，调用了runtime.convT2I，这个方法返回的类型是iface func convT2I(tab *itab, elem unsafe.Pointer) (i iface) { t := tab._type if raceenabled { raceReadObjectPC(t, elem, getcallerpc(), funcPC(convT2I)) } if msanenabled { msanread(elem, t.size) } x := mallocgc(t.size, t, true) typedmemmove(t, x, elem) i.tab = tab i.data = x return } itab包括具体值的type和 interface 的type，还有其他字段 type itab struct { inter *interfacetype // 接口定义的类型 _type *_type // 接口指向具体值的 type hash uint32 // 类型的hash值 _ [4]byte fun [1]uintptr // 判断接口是否实现所有方法（下面会讲到） } 在itab结构体的init方法中，是所有字段的初始化，重点看这个方法： func (m *itab) init() string { inter := m.inter typ := m._type x := typ.uncommon() // 在 interfacetype 的结构体中，mhdr 存着所有需要实现的方法的 // 结构体切片 []imethod，都是按照方法名的字典序排列的，其中： // ni 是全量的方法（所有要实现的方法）的个数 // nt 是已实现的方法的个数 ni := len(inter.mhdr) nt := int(x.mcount) xmhdr := (*[1 &lt;&lt; 16]method)(add(unsafe.Pointer(x), uintptr(x.moff)))[:nt:nt] j := 0 methods := (*[1 &lt;&lt; 16]unsafe.Pointer)(unsafe.Pointer(&amp;m.fun[0]))[:ni:ni] var fun0 unsafe.Pointer imethods: for k := 0; k &lt; ni; k++ { // 从第一个开始，逐个对比 i := &amp;inter.mhdr[k] itype := inter.typ.typeOff(i.ityp) name := inter.typ.nameOff(i.name) iname := name.name() ipkg := name.pkgPath() if ipkg == &quot;&quot; { ipkg = inter.pkgpath.name() } for ; j &lt; nt; j++ { t := &amp;xmhdr[j] tname := typ.nameOff(t.name) // 比较已实现方法的 type 和 name 是否一致 if typ.typeOff(t.mtyp) == itype &amp;&amp; tname.name() == iname { pkgPath := tname.pkgPath() if pkgPath == &quot;&quot; { pkgPath = typ.nameOff(x.pkgpath).name() } if tname.isExported() || pkgPath == ipkg { if m != nil { // 计算每个 method 对应代码块的内存地址 ifn := typ.textOff(t.ifn) if k == 0 { fun0 = ifn // we&#39;ll set m.fun[0] at the end } else { methods[k] = ifn } } continue imethods } } } // 如果没有找到，将 func[0] 设置为0，返回该实现的 method 的 name m.fun[0] = 0 return iname } // 第一个方法的 ptr 和 type 的 hash m.fun[0] = uintptr(fun0) m.hash = typ.hash return &quot;&quot; } itabTable还有一种将interface类型的实现，赋值给另外一个interface： TEXT main.main(SB) /Users/such/gomodule/runtime/main.go ...省略部分指令 main.go:18 0x10b71f5 488d842480000000 lea rax, ptr [rsp+0x80] main.go:18 0x10b71fd 4889442408 mov qword ptr [rsp+0x8], rax main.go:18 0x10b7202 e84921f5ff call $runtime.convT2I func convI2I(inter *interfacetype, i iface) (r iface) { tab := i.tab if tab == nil { return } if tab.inter == inter { r.tab = tab r.data = i.data return } r.tab = getitab(inter, tab._type, false) r.data = i.data return } 通过前面的分析，我们又知道， iface 是由 tab 和 data 两个字段组成。所以，实际上 convI2I 函数真正要做的事，找到新 interface 的 tab 和 data，就大功告成了。在iface.go 文件头部定义了itabTable全局哈希表存所有itab，其实就是空间换时间的思想。itabTable是itabTableType结构体（我的golang版本是1.12.7） type itabTableType struct { size uintptr // 大小，2的幂 count uintptr // 已有的 itab entry 个数 entries [itabInitSize]*itab // 保存 itab entry } getitabgetitab是查找itab的方法 func getitab(inter *interfacetype, typ *_type, canfail bool) *itab { if len(inter.mhdr) == 0 { throw(&quot;internal error - misuse of itab&quot;) } if typ.tflag&amp;tflagUncommon == 0 { if canfail { return nil } name := inter.typ.nameOff(inter.mhdr[0].name) panic(&amp;TypeAssertionError{nil, typ, &amp;inter.typ, name.name()}) } var m *itab t := (*itabTableType)(atomic.Loadp(unsafe.Pointer(&amp;itabTable))) if m = t.find(inter, typ); m != nil { goto finish } // Not found. Grab the lock and try again. lock(&amp;itabLock) if m = itabTable.find(inter, typ); m != nil { unlock(&amp;itabLock) goto finish } // Entry doesn&#39;t exist yet. Make a new entry &amp; add it. m = (*itab)(persistentalloc(unsafe.Sizeof(itab{})+uintptr(len(inter.mhdr)-1)*sys.PtrSize, 0, &amp;memstats.other_sys)) m.inter = inter m._type = typ m.init() itabAdd(m) unlock(&amp;itabLock) finish: if m.fun[0] != 0 { return m } if canfail { return nil } // 如果不是 &quot;_, ok := &quot; 类型的断言，会有panic panic(&amp;TypeAssertionError{concrete: typ, asserted: &amp;inter.typ, missingMethod: m.init()}) } 会调用find方法，根据interfacetype和_type的 hash 值，在itabTable中查找，找到的话直接返回；否则，生成新的itab，加入 itabTable 中。有个问题，就是为什么第一次不加锁找，而第二次加锁？我个人的理解是：首先：应该还是想避免锁的开销（之前在滴滴有幸听过曹大分享【内存重排】，对常用package在concurrently时，锁引起的问题做了一些分析。）， 而第二次加锁，我觉得更多的是在未找到 itab 后，会新生成一个 itab 写入全局哈希表中，如果有其他协程在查询时，也未找到，可以并发安全写入。 itabAddfunc itabAdd(m *itab) { if getg().m.mallocing != 0 { throw(&quot;malloc deadlock&quot;) } t := itabTable if t.count &gt;= 3*(t.size/4) { // 75% load factor t2 := (*itabTableType)(mallocgc((2+2*t.size)*sys.PtrSize, nil, true)) t2.size = t.size * 2 iterate_itabs(t2.add) if t2.count != t.count { throw(&quot;mismatched count during itab table copy&quot;) } atomicstorep(unsafe.Pointer(&amp;itabTable), unsafe.Pointer(t2)) t = itabTable } t.add(m) } itabAdd 是添加itab加入itabTable的方法。既然是hash表，就一定会发生扩容。每次都是2的倍数的增长，创建新的 itabTable 再原子的替换。在 iterate_itabs（复制）时，并未加锁，这里不是协程安全的，而是在添加前，在getitab方法中有锁的操作，会等待复制完成。","categories":[],"tags":[{"name":"go","slug":"go","permalink":"http://8090lambert.cn/tags/go/"}]},{"title":"Redis5.0 RDB文件超详细解析","slug":"Redis5.0 RDB文件超详细解析","date":"2019-05-26T12:14:26.000Z","updated":"2021-05-09T14:50:16.596Z","comments":true,"path":"2019/05/26/Redis5.0 RDB文件超详细解析/","link":"","permalink":"http://8090lambert.cn/2019/05/26/Redis5.0 RDB文件超详细解析/","excerpt":"","text":"Redis RDB介绍RDB 是 Redis 将 server 端的内存中的 k/v 对以二进制的方式，持久化存储的一种文件形式。文件中，一般会以 对象的长度+对象 的格式来存储，只要根据这个格式，就能渐进的遍历整个文件。Redis 还支持开启 LZF 的压缩算法，可以牺牲CPU时间，来减少 RDB 文件的大小；如果开启LZF并且超过20个bytes时，会将压缩后的字符写入文件。 RDB文件格式➜ go-redis-parser od -A x -t x1c -v ./teststub/dumpV9.rdb 000000 52 45 44 49 53 30 30 30 39 fa 09 72 65 64 69 73 R E D I S 0 0 0 9 372 \\t r e d i s 000010 2d 76 65 72 05 35 2e 30 2e 35 fa 0a 72 65 64 69 - v e r 005 5 . 0 . 5 372 \\n r e d i 000020 73 2d 62 69 74 73 c0 40 fa 05 63 74 69 6d 65 c2 s - b i t s 300 @ 372 005 c t i m e 302 000030 71 8a 8d 5d fa 08 75 73 65 64 2d 6d 65 6d c2 30 q 212 215 ] 372 \\b u s e d - m e m 302 0 000040 e0 0f 00 fa 0c 61 6f 66 2d 70 72 65 61 6d 62 6c 340 017 \\0 372 \\f a o f - p r e a m b l 000050 65 c0 00 fe 00 fb 06 00 f9 00 00 01 73 01 61 f9 e 300 \\0 376 \\0 373 006 \\0 371 \\0 \\0 001 s 001 a 371 000060 03 0e 02 6c 69 01 11 11 00 00 00 0d 00 00 00 02 003 016 002 l i 001 021 021 \\0 \\0 \\0 \\r \\0 \\0 \\0 002 000070 00 00 01 61 03 01 62 ff f9 00 02 03 73 65 74 02 \\0 \\0 001 a 003 001 b 377 371 \\0 002 003 s e t 002 000080 01 62 01 61 f9 00 0f 06 73 74 72 65 61 6d 01 10 001 b 001 a 371 \\0 017 006 s t r e a m 001 020 000090 00 00 01 6d 70 b5 4a 7e 00 00 00 00 00 00 00 00 \\0 \\0 001 m p 265 J ~ \\0 \\0 \\0 \\0 \\0 \\0 \\0 \\0 0000a0 40 52 52 00 00 00 18 00 03 01 00 01 02 01 84 6e @ R R \\0 \\0 \\0 030 \\0 003 001 \\0 001 002 001 204 n 0000b0 61 6d 65 05 83 61 67 65 04 00 01 02 01 00 01 00 a m e 005 203 a g e 004 \\0 001 002 001 \\0 001 \\0 0000c0 01 87 4c 61 6d 62 65 72 74 08 1d 01 05 01 02 01 001 207 L a m b e r t \\b 035 001 005 001 002 001 0000d0 f2 33 8c 00 04 00 01 84 4a 61 63 6b 05 1a 01 05 362 3 214 \\0 004 \\0 001 204 J a c k 005 032 001 005 0000e0 01 02 01 f2 9c ad 00 04 00 01 83 54 6f 6d 04 1e 001 002 001 362 234 255 \\0 004 \\0 001 203 T o m 004 036 0000f0 01 05 01 ff 03 81 00 00 01 6d 70 b5 f8 1a 00 01 001 005 001 377 003 201 \\0 \\0 001 m p 265 370 032 \\0 001 000100 05 67 72 6f 75 70 00 00 00 00 f9 00 0c 04 7a 73 005 g r o u p \\0 \\0 \\0 \\0 371 \\0 \\f 004 z s 000110 65 74 15 15 00 00 00 12 00 00 00 04 00 00 01 61 e t 025 025 \\0 \\0 \\0 022 \\0 \\0 \\0 004 \\0 \\0 001 a 000120 03 f2 02 01 62 03 f3 ff f9 03 0d 01 68 11 11 00 003 362 002 001 b 003 363 377 371 003 \\r 001 h 021 021 \\0 000130 00 00 0d 00 00 00 02 00 00 01 61 03 01 61 ff ff \\0 \\0 \\r \\0 \\0 \\0 002 \\0 \\0 001 a 003 001 a 377 377 000140 0e e0 f7 31 2f 37 16 df 016 340 367 1 / 7 026 337 000148 这是一个 version 9 的 RDB 文件 魔数 Magic Number文件前 9 个字节是一个 魔数，5 个字节REDIS 和 4 个字节的版本号 009。 辅助字段 Aux Fields通用字符串字段，用于向RDB添加状态，Version 7 加入的，向后兼容。AUX字段由两个字符串组成：键和值。整理了下，除了 lua，有这些字段： redis-ver：版本号 redis-bits：OS Arch ctime：RDB文件创建时间 used-mem：使用内存大小 repl-stream-db：在server.master客户端中选择的数据库 repl-id：当前实例 replication ID repl-offset：当前实例复制的偏移量 数据库索引fe(0xfe)，fb(0xfb)是 10 进制的 254 和 251，在 RDB 分别对应着，SELECT_DB和RESIZE_DB。 每一个SELECTDB后都会紧跟着RESIZEDB，后者表示的是当前数据库hashtable键大小的提示，每次切换数据库时提前读到，避免不必要的rehash。 数据库键值对接下来，读到的就是Redis中所有存储着的K/V对： LFU/LRU Idle后面f9(0xf9)是 10 进制的 249，是表示 key 对象的lfu_idle，这个字段是只有开启maxmemory-policy并且设置为volatile-lfu或allkeys-lfu才会写入文件。LRU的同理，不过前缀是 f8(0xf8)，maxmemory-policy要设置为：allkeys-lru和volatile-lru。 String结构00(0x00)是 10 进制的 0，表示string类型的对象，01 指key是一个字节长度：“s”, value 也是一个字节长度：“a”, List结构0e(0x0e)是 10 进制的 14，表示list类型的对象（在3.2版本之前，是由ziplist和linkedlist结构保存，之后存储是quicklist）；2个字节长度的key：”li”,下来分别是一个长度的items：”a” 和 “b” Set结构2，表示Set类型对象，3个长度 “set”，两个members：”a” 和 “b” Stream结构继 module 之后，redis 在 5.0 增加了新的数据类型 Stream，不了解的同学可以Google下，很多介绍的文章。根据作者自己说，它也是充分借鉴了 kafka 的设计思想，在已有 list 的基础上增加了另一种流式类型。基本。0f(0x0f)是 10 进制的 15，是Stream类型的对象，5个长度的key:”stream”。然后是 StreamId结构体，6d 70 b5 4a 7e分别是 10 进制的（109 112 181 74 126），二进制 01101101 01110000 10110101 01001010 01111110，因为毫秒是uint，加上符号位即：1 01101101 01110000 10110101 01001010 01111110 对应如图所示:first-entry 恰好是创建stream的毫秒数，后面跟着8位的随机数0；然后是entry结构，每个entry结构前面，也会有和streamId相同的messageId，这里就不具体逐位分析了。长度是 3，第一个字段：”name”;第二个字段：”age”，3个entry分别是name:Lambert,age:29、name:Jack,age:26、name:Tom,age:30；下来是消费组Group，名为：group 的消费组，因为没有任何消费，所以偏移量pending entry list都是0。 ZSet结构0c 是 10 进制的 12，以ziplist结构存储的Sortedset类型，4个长度的key：”zset”，接下来的 4 表示：4个元素，zset会将member和score一起保存，所以，就是2组members；分别是：{memeber:&quot;a&quot;,score:1}、{member:&quot;b&quot;,score:2}。 Hash结构0d(13) 是RDB_TYPE_HASH_ZIPLIST，表示是ziplist存储的hash类型数据，1 个长度的key：”h”。key后面的2，存着hash结构的field和value；在 field 和 value 前面的 1，分别是指各自的长度，都是 “a” 文件EOFff(255) EOF，在所有数据写完结束后，会以一个EOF结尾 CheckSum从Version 5 开始，如果在配置文件中开启rdbchecksum yes，会在RDB文件的结尾处，用 8 个字节保存通过CRC64计算整个文件内容的检验和。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://8090lambert.cn/tags/redis/"}]},{"title":"从底层理解 Golang 的 map 实现","slug":"从底层理解 Golang 的 map 实现","date":"2019-04-30T09:34:15.000Z","updated":"2021-05-09T14:50:16.598Z","comments":true,"path":"2019/04/30/从底层理解 Golang 的 map 实现/","link":"","permalink":"http://8090lambert.cn/2019/04/30/从底层理解 Golang 的 map 实现/","excerpt":"","text":"定义golang 中的 map 就是常用的 hashtable，底层实现由 hmap，维护着若干个 bucket 数组，通常每个 bucket 保存着8组kv对，如果超过8个(发生hash冲突时)，会在 extra 字段结构体中的 overflow ，使用链地址法一直扩展下去。先看下 hmap 结构体： type hmap struct { count int // 元素的个数 flags uint8 // 标记读写状态，主要是做竞态检测，避免并发读写 B uint8 // 可以容纳 2 ^ N 个bucket noverflow uint16 // 溢出的bucket个数 hash0 uint32 // hash 因子 buckets unsafe.Pointer // 指向数组buckets的指针 oldbuckets unsafe.Pointer // growing 时保存原buckets的指针 nevacuate uintptr // growing 时已迁移的个数 extra *mapextra } type mapextra struct { overflow *[]*bmap oldoverflow *[]*bmap nextOverflow *bmap } bucket 的结构体： // A bucket for a Go map. type bmap struct { // tophash generally contains the top byte of the hash value // for each key in this bucket. If tophash[0] &lt; minTopHash, // tophash[0] is a bucket evacuation state instead. tophash [bucketCnt]uint8 // 记录着每个key的高8个bits // Followed by bucketCnt keys and then bucketCnt elems. // NOTE: packing all the keys together and then all the elems together makes the // code a bit more complicated than alternating key/elem/key/elem/... but it allows // us to eliminate padding which would be needed for, e.g., map[int64]int8. // Followed by an overflow pointer. } 其中 kv 对是按照 key0/key1/key2/…val0/val1/val2/… 的格式排列，虽然在保存上面会比key/value对更复杂一些，但是避免了因为cpu要求固定长度读取，字节对齐，造成的空间浪费。 初始化 &amp;&amp; 插入package main func main() { a := map[string]int{&quot;one&quot;: 1, &quot;two&quot;: 2, &quot;three&quot;: 3} _ = a[&quot;one&quot;] } 初始化3个key/value的map TEXT main.main(SB) /Users/such/gomodule/runtime/main.go =&gt; main.go:3 0x10565fb* 4881ec70010000 sub rsp, 0x170 main.go:3 0x1056602 4889ac2468010000 mov qword ptr [rsp+0x168], rbp main.go:3 0x105660a 488dac2468010000 lea rbp, ptr [rsp+0x168] main.go:4 0x105664b 488b6d00 mov rbp, qword ptr [rbp] main.go:4 0x105666d e8de9cfeff call $runtime.fastrand main.go:4 0x1056672 488b442450 mov rax, qword ptr [rsp+0x50] main.go:4 0x1056677 8400 test byte ptr [rax], al main.go:4 0x10566c6 48894c2410 mov qword ptr [rsp+0x10], rcx main.go:4 0x10566cb 4889442418 mov qword ptr [rsp+0x18], rax main.go:4 0x10566d0 e80b8efbff call $runtime.mapassign_faststr main.go:4 0x1056726 48894c2410 mov qword ptr [rsp+0x10], rcx main.go:4 0x105672b 4889442418 mov qword ptr [rsp+0x18], rax main.go:4 0x1056730 e8ab8dfbff call $runtime.mapassign_faststr main.go:4 0x1056786 4889442410 mov qword ptr [rsp+0x10], rax main.go:4 0x105678b 48894c2418 mov qword ptr [rsp+0x18], rcx main.go:4 0x1056790 e84b8dfbff call $runtime.mapassign_faststr (省略了部分) 可以看出来，声明时连续调用三次 call $runtime.mapassign_faststr 添加键值对 func mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer { if h == nil { panic(plainError(&quot;assignment to entry in nil map&quot;)) } if raceenabled { callerpc := getcallerpc() pc := funcPC(mapassign) racewritepc(unsafe.Pointer(h), callerpc, pc) raceReadObjectPC(t.key, key, callerpc, pc) } // 看到这里，发现和之前 slice 声明时一样，都会做竞态检测 if msanenabled { msanread(key, t.key.size) } // 这里就是并发读写map时，panic的地方 if h.flags&amp;hashWriting != 0 { throw(&quot;concurrent map writes&quot;) } // t 是 map 的类型，因此在编译时，可以确定key的类型，继而确定hash算法。 alg := t.key.alg hash := alg.hash(key, uintptr(h.hash0)) // 设置flag为writing h.flags ^= hashWriting if h.buckets == nil { h.buckets = newobject(t.bucket) // newarray(t.bucket, 1) } again: // 重新计算bucket的hash bucket := hash &amp; bucketMask(h.B) if h.growing() { growWork(t, h, bucket) } b := (*bmap)(unsafe.Pointer(uintptr(h.buckets) + bucket*uintptr(t.bucketsize))) top := tophash(hash) var inserti *uint8 var insertk unsafe.Pointer var elem unsafe.Pointer bucketloop: // 遍历找到bucket for { for i := uintptr(0); i &lt; bucketCnt; i++ { if b.tophash[i] != top { if isEmpty(b.tophash[i]) &amp;&amp; inserti == nil { inserti = &amp;b.tophash[i] insertk = add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) elem = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) } if b.tophash[i] == emptyRest { break bucketloop } continue } k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if t.indirectkey() { k = *((*unsafe.Pointer)(k)) } // equal 方法也是根据不同的数据类型，在编译时确定 if !alg.equal(key, k) { continue } // map 中已经存在 key，修改 key 对应的 value if t.needkeyupdate() { typedmemmove(t.key, k, key) } elem = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) goto done } ovf := b.overflow(t) if ovf == nil { break } b = ovf } // Did not find mapping for key. Allocate new cell &amp; add entry. // If we hit the max load factor or we have too many overflow buckets, // and we&#39;re not already in the middle of growing, start growing. if !h.growing() &amp;&amp; (overLoadFactor(h.count+1, h.B) || tooManyOverflowBuckets(h.noverflow, h.B)) { hashGrow(t, h) goto again // Growing the table invalidates everything, so try again } if inserti == nil // 如果没有找到插入的node，即当前所有桶都已放满 newb := h.newoverflow(t, b) inserti = &amp;newb.tophash[0] insertk = add(unsafe.Pointer(newb), dataOffset) elem = add(insertk, bucketCnt*uintptr(t.keysize)) } // store new key/elem at insert position if t.indirectkey() { kmem := newobject(t.key) *(*unsafe.Pointer)(insertk) = kmem insertk = kmem } if t.indirectelem() { vmem := newobject(t.elem) *(*unsafe.Pointer)(elem) = vmem } typedmemmove(t.key, insertk, key) *inserti = top h.count++ done: // 再次检查（双重校验锁的思路）是否并发写 if h.flags&amp;hashWriting == 0 { throw(&quot;concurrent map writes&quot;) } h.flags &amp;^= hashWriting if t.indirectelem() { elem = *((*unsafe.Pointer)(elem)) } return elem } 查找TEXT main.main(SB) /Users/such/gomodule/runtime/main.go =&gt; main.go:6 0x10567a9* 488d0550e10000 lea rax, ptr [rip+0xe150] main.go:6 0x10567c5 4889442410 mov qword ptr [rsp+0x10], rax main.go:6 0x10567ca 48c744241803000000 mov qword ptr [rsp+0x18], 0x3 main.go:6 0x10567d3 e89885fbff call $runtime.mapaccess1_faststr 在 map 中找一个 key 的时候，runtime 调用了 mapaccess1 方法，和添加时很类似 func mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer { if raceenabled &amp;&amp; h != nil { callerpc := getcallerpc() pc := funcPC(mapaccess1) racereadpc(unsafe.Pointer(h), callerpc, pc) raceReadObjectPC(t.key, key, callerpc, pc) } if msanenabled &amp;&amp; h != nil { msanread(key, t.key.size) } if h == nil || h.count == 0 { if t.hashMightPanic() { t.key.alg.hash(key, 0) // see issue 23734 } return unsafe.Pointer(&amp;zeroVal[0]) } if h.flags&amp;hashWriting != 0 { throw(&quot;concurrent map read and map write&quot;) } alg := t.key.alg hash := alg.hash(key, uintptr(h.hash0)) m := bucketMask(h.B) b := (*bmap)(add(h.buckets, (hash&amp;m)*uintptr(t.bucketsize))) if c := h.oldbuckets; c != nil { if !h.sameSizeGrow() { // There used to be half as many buckets; mask down one more power of two. m &gt;&gt;= 1 } oldb := (*bmap)(add(c, (hash&amp;m)*uintptr(t.bucketsize))) if !evacuated(oldb) { b = oldb } } top := tophash(hash) bucketloop: for ; b != nil; b = b.overflow(t) { for i := uintptr(0); i &lt; bucketCnt; i++ { if b.tophash[i] != top { if b.tophash[i] == emptyRest { break bucketloop } continue } k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if t.indirectkey() { k = *((*unsafe.Pointer)(k)) } // 如果找到 key，就返回 key 指向的 value 指针的值， // 在计算 ptr 的时候，初始位置当前bmap, 偏移量 offset，是一个 bmap 结构体的大小，但对于amd64架构， // 还需要考虑字节对齐，即 8 字节对齐（dataOffset）+ 8个key的大小 + i (当前索引) 个value的大小 if alg.equal(key, k) { e := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) if t.indirectelem() { e = *((*unsafe.Pointer)(e)) } return e } } } // 如果未找到的话，返回零对象的引用的指针 return unsafe.Pointer(&amp;zeroVal[0]) } 在 map 包里，还有个类似的方法， mapaccess2 在经过验证，在 _, ok := a[&quot;one&quot;]一般用于判断key是否存在的写法时，是会用到。其实根据函数的返回值也可以看出。 Growing和 slice 一样，在 map 的元素持续增长时，每个bucket极端情况下会有很多overflow，退化成链表，需要 rehash。一般扩容是在 h.count &gt; loadFactor(2^B)。负载因子一般是：容量 / bucket数量，golang 的负载因子 loadFactorNum / loadFactorDen = 6.5，为什么不选择1呢，像 Redis 的 dictentry，只能保存一组键值对，golang的话，一个bucket正常情况下可以保存8组键值对；那为什么选择6.5这个值呢，作者给出了一组数据。 loadFactor %overflow bytes/entry hitprobe missprobe 4.00 2.13 20.77 3.00 4.00 4.50 4.05 17.30 3.25 4.50 5.00 6.85 14.77 3.50 5.00 5.50 10.55 12.94 3.75 5.50 6.00 15.27 11.67 4.00 6.00 6.50 20.90 10.79 4.25 6.50 7.00 27.14 10.15 4.50 7.00 7.50 34.03 9.73 4.75 7.50 8.00 41.10 9.40 5.00 8.00 loadFactor：负载因子；%overflow：溢出率，有溢出 bucket 的占比；bytes/entry：每个 key/value 对占用字节比；hitprobe：找到一个存在的key平均查找个数；missprobe：找到一个不存在的key平均查找个数； 通常在负载因子 &gt; 6.5时，就是平均每个bucket存储的键值对超过6.5个或者是overflow的数量 &gt; 2 ^ 15时会发生扩容（迁移）。它分为两种情况：第一种：由于map在不断的insert 和 delete 中，bucket中的键值存储不够均匀，内存利用率很低，需要进行迁移。（注：bucket数量不做增加）第二种：真正的，因为负载因子过大引起的扩容，bucket 增加为原 bucket 的两倍不论上述哪一种 rehash，都是调用 hashGrow 方法： 定义原 hmap 中指向 buckets 数组的指针 创建 bucket 数组并设置为 hmap 的 bucket 字段 将 extra 中的 oldoverflow 指向 overflow，overflow 指向 nil 如果正在 growing 的话，开始渐进式的迁移，在 growWork 方法里是 bucket 中 key/value 的迁移 在全部迁移完成后，释放内存 注意： golang在rehash时，和Redis一样采用渐进式的rehash，没有一次性迁移所有的buckets，而是把key的迁移分摊到每次插入或删除时，在 bucket 中的 key/value 全部迁移完成释放oldbucket和extra.oldoverflow（尽可能不去使用map存储大量数据；最好在初始化一次性声明cap，避免频繁扩容） 删除func mapdelete(t *maptype, h *hmap, key unsafe.Pointer) { ...省略 search: for ; b != nil; b = b.overflow(t) { for i := uintptr(0); i &lt; bucketCnt; i++ { if t.indirectkey() { *(*unsafe.Pointer)(k) = nil } else if t.key.ptrdata != 0 { memclrHasPointers(k, t.key.size) } e := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) if t.indirectelem() { *(*unsafe.Pointer)(e) = nil } else if t.elem.ptrdata != 0 { memclrHasPointers(e, t.elem.size) } else { memclrNoHeapPointers(e, t.elem.size) } b.tophash[i] = emptyOne if i == bucketCnt-1 { if b.overflow(t) != nil &amp;&amp; b.overflow(t).tophash[0] != emptyRest { goto notLast } } else { if b.tophash[i+1] != emptyRest { goto notLast } } for { b.tophash[i] = emptyRest if i == 0 { if b == bOrig { break // beginning of initial bucket, we&#39;re done. } // Find previous bucket, continue at its last entry. c := b for b = bOrig; b.overflow(t) != c; b = b.overflow(t) { } i = bucketCnt - 1 } else { i-- } if b.tophash[i] != emptyOne { break } } notLast: h.count-- break search } } ... } key 和value，如果是值类型的话，直接设置为nil, 如果是指针的话，就从 ptr 位置开始清除 n 个bytes;接着在删除时，只是在tophash对应的位置上，设置为 empty 的标记（b.tophash[i] = emptyOne），没有真正的释放内存空间，因为频繁的申请、释放内存空间开销很大，如果真正想释放的话，只有依赖GC；如果bucket是以一些 emptyOne 的标记结束，最终，就设置为 emptyRest 标记，emptyOne 和 emptyRest 都是空的标记，emptyRest的区别就是：标记在 高索引位 和 overflow bucket 都是空的，应该是考虑在之后重用时，插入和删除操作需要查找位置时，减少查找次数。 建议做两组试验，第一组是：提前分配好 map 的总容量后追加k/v；另一组是：初始化 0 容量的 map 后做追加 package main import &quot;testing&quot; var count int = 100000 func addition(m map[int]int) map[int]int { for i := 0; i &lt; count; i++ { m[i] = i } return m } func BenchmarkGrows(b *testing.B) { b.ResetTimer() for i := 0; i &lt; b.N; i++ { m := make(map[int]int) addition(m) } } func BenchmarkNoGrows(b *testing.B) { b.ResetTimer() for i := 0; i &lt; b.N; i++ { m := make(map[int]int, count) addition(m) } } $ go test -bench=. ./ goos: darwin goarch: amd64 # benchmark名字 -CPU数 执行次数 平均执行时间ns BenchmarkGrows-4 200 8298505 ns/op BenchmarkNoGrows-4 300 4627118 ns/op PASS ok _/Users/such/gomodule/runtime 4.401s 提前定义容量的case平均执行时间比未定义容量的快了80% — 扩容时的数据拷贝和重新哈希成本很高！再看看内存的分配次数： $ go test -bench=. -benchmem ./ goos: darwin goarch: amd64 # benchmark名字 -CPU数 执行次数 平均执行时间ns 每次分配内存大小 每次内存分配次数 BenchmarkGrows-4 200 9265553 ns/op 5768155 B/op 4010 allocs/op BenchmarkNoGrows-4 300 4855000 ns/op 2829115 B/op 1678 allocs/op PASS ok _/Users/such/gomodule/runtime 4.704s 两个方法执行相同的次数，GC的次数也会多出一倍 func main() { for i := 0; i &lt; 5; i++ { n := make(map[int]int, count) addition(n) //m := make(map[int]int) //addition(m) } } // 第一组，预分配 $ go build -o growth &amp;&amp; GODEBUG=gctrace=1 ./growth gc 1 @0.006s 0%: 0.002+0.091+0.015 ms clock, 0.011+0.033/0.011/0.088+0.060 ms cpu, 5-&gt;5-&gt;2 MB, 6 MB goal, 4 P gc 2 @0.012s 0%: 0.001+0.041+0.002 ms clock, 0.007+0.032/0.007/0.033+0.009 ms cpu, 5-&gt;5-&gt;2 MB, 6 MB goal, 4 P gc 3 @0.017s 0%: 0.002+0.090+0.010 ms clock, 0.008+0.035/0.006/0.084+0.041 ms cpu, 5-&gt;5-&gt;2 MB, 6 MB goal, 4 P gc 4 @0.022s 0%: 0.001+0.056+0.008 ms clock, 0.007+0.026/0.003/0.041+0.034 ms cpu, 5-&gt;5-&gt;2 MB, 6 MB goal, 4 P // 第二组，未分配 $ go build -o growth &amp;&amp; GODEBUG=gctrace=1 ./growth gc 1 @0.005s 0%: 0.001+0.10+0.001 ms clock, 0.007+0.076/0.004/0.13+0.007 ms cpu, 5-&gt;5-&gt;3 MB, 6 MB goal, 4 P gc 2 @0.012s 0%: 0.002+0.071+0.010 ms clock, 0.008+0.016/0.010/0.075+0.040 ms cpu, 5-&gt;5-&gt;0 MB, 7 MB goal, 4 P gc 3 @0.015s 0%: 0.001+0.13+0.009 ms clock, 0.007+0.006/0.037/0.082+0.036 ms cpu, 4-&gt;5-&gt;3 MB, 5 MB goal, 4 P gc 4 @0.021s 0%: 0.001+0.13+0.009 ms clock, 0.007+0.040/0.007/0.058+0.038 ms cpu, 6-&gt;6-&gt;1 MB, 7 MB goal, 4 P gc 5 @0.024s 0%: 0.001+0.084+0.001 ms clock, 0.005+0.036/0.006/0.052+0.006 ms cpu, 4-&gt;4-&gt;3 MB, 5 MB goal, 4 P gc 6 @0.030s 0%: 0.002+0.075+0.001 ms clock, 0.008+0.056/0.004/0.072+0.007 ms cpu, 6-&gt;6-&gt;1 MB, 7 MB goal, 4 P gc 7 @0.033s 0%: 0.013+0.11+0.003 ms clock, 0.053+0.047/0.013/0.075+0.012 ms cpu, 4-&gt;4-&gt;3 MB, 5 MB goal, 4 P gc 8 @0.041s 0%: 0.002+0.073+0.024 ms clock, 0.008+0.033/0.010/0.067+0.097 ms cpu, 6-&gt;6-&gt;1 MB, 7 MB goal, 4 P gc 9 @0.043s 0%: 0.001+0.067+0.001 ms clock, 0.006+0.046/0.003/0.070+0.006 ms cpu, 4-&gt;4-&gt;3 MB, 5 MB goal, 4 P 有个1千万kv的 map，测试在什么情况下会回收内存 package main var count = 10000000 var dict = make(map[int]int, count) func addition() { for i := 0; i &lt; count; i++ { dict[i] = i } } func clear() { for k := range dict { delete(dict, k) } //dict = nil } func main() { addition() clear() debug.FreeOSMemory() } $ go build -o clear &amp;&amp; GODEBUG=gctrace=1 ./clear gc 1 @0.007s 0%: 0.006+0.12+0.015 ms clock, 0.025+0.037/0.038/0.12+0.061 ms cpu, 306-&gt;306-&gt;306 MB, 307 MB goal, 4 P gc 2 @0.963s 0%: 0.004+1.0+0.025 ms clock, 0.017+0/0.96/0.48+0.10 ms cpu, 307-&gt;307-&gt;306 MB, 612 MB goal, 4 P gc 3 @1.381s 0%: 0.004+0.081+0.003 ms clock, 0.018+0/0.051/0.086+0.013 ms cpu, 309-&gt;309-&gt;306 MB, 612 MB goal, 4 P (forced) scvg-1: 14 MB released scvg-1: inuse: 306, idle: 77, sys: 383, released: 77, consumed: 306 (MB) 删除了所有kv，堆大小（goal）并无变化 func clear() { for k := range dict { delete(dict, k) } dict = nil } $ go build -o clear &amp;&amp; GODEBUG=gctrace=1 ./clear gc 1 @0.006s 0%: 0.004+0.12+0.010 ms clock, 0.019+0.035/0.016/0.17+0.043 ms cpu, 306-&gt;306-&gt;306 MB, 307 MB goal, 4 P gc 2 @0.942s 0%: 0.003+1.0+0.010 ms clock, 0.012+0/0.85/0.54+0.043 ms cpu, 307-&gt;307-&gt;306 MB, 612 MB goal, 4 P gc 3 @1.321s 0%: 0.003+0.072+0.002 ms clock, 0.013+0/0.050/0.090+0.010 ms cpu, 309-&gt;309-&gt;0 MB, 612 MB goal, 4 P (forced) scvg-1: 319 MB released scvg-1: inuse: 0, idle: 383, sys: 383, released: 383, consumed: 0 (MB) 清除过后，设置为nil，才会真正释放内存。（本身每2分钟强制 runtime.GC()，每5分钟 scavenge 释放内存，其实不必太过纠结是否真正释放，未真正释放也是为了后面有可能的重用，但有时需要真实释放时，清楚怎么做才能解决问题） ReferenceMap：https://golang.org/src/runtime/map.go?h=hmap#L115Benchmark：https://dave.cheney.net/2013/06/30/how-to-write-benchmarks-in-goGctrace：https://dave.cheney.net/tag/godebugFreeOsMemory：https://golang.org/pkg/runtime/debug/#FreeOSMemory","categories":[],"tags":[{"name":"go","slug":"go","permalink":"http://8090lambert.cn/tags/go/"}]},{"title":"详解 Golang 的 slice 设计与实现","slug":"详解 Golang 的 slice 设计与实现","date":"2019-04-21T06:50:13.000Z","updated":"2021-05-09T14:50:16.599Z","comments":true,"path":"2019/04/21/详解 Golang 的 slice 设计与实现/","link":"","permalink":"http://8090lambert.cn/2019/04/21/详解 Golang 的 slice 设计与实现/","excerpt":"","text":"Slice 结构体slice 是 golang 中利用指针指向某个连续片段的数组，所以本质上它算是引用类型。一个 slice 在 golang 中占用24个 bytes a = make([]int, 0) unsafe.Sizeof(a) // 24 var c int unsafe.Sizeof(c) // 8, 一个 int 在 golang 中占用 8 个bytes(本机是64位操作系统) 在 runtime 的 slice.go 中，定义了 slice 的 struct type slice struct { array unsafe.Pointer // 8 bytes len int // 8 bytes cap int // 8 bytes // 确认了，slice 的大小 24 } array 是指向真实的数组的 ptr len 是指切片已有元素个数 cap 是指当前分配的空间 准备调试简单准备一段程序，看看 golang 是如何初始化一个切片的 package main import &quot;fmt&quot; func main() { a := make([]int, 0) a = append(a, 2, 3, 4) fmt.Println(a) } Slice 初始化使用 dlv 调试，反汇编后： (dlv) disassemble TEXT main.main(SB) /Users/such/gomodule/runtime/main.go main.go:5 0x10b70f0 65488b0c2530000000 mov rcx, qword ptr gs:[0x30] main.go:5 0x10b70f9 488d4424e8 lea rax, ptr [rsp-0x18] main.go:5 0x10b70fe 483b4110 cmp rax, qword ptr [rcx+0x10] main.go:5 0x10b7102 0f8637010000 jbe 0x10b723f main.go:5 0x10b7108* 4881ec98000000 sub rsp, 0x98 main.go:5 0x10b710f 4889ac2490000000 mov qword ptr [rsp+0x90], rbp main.go:5 0x10b7117 488dac2490000000 lea rbp, ptr [rsp+0x90] main.go:6 0x10b711f 488d051a0e0100 lea rax, ptr [rip+0x10e1a] main.go:6 0x10b7126 48890424 mov qword ptr [rsp], rax main.go:6 0x10b712a 0f57c0 xorps xmm0, xmm0 main.go:6 0x10b712d 0f11442408 movups xmmword ptr [rsp+0x8], xmm0 main.go:6 0x10b7132 e8b99af8ff ** call $runtime.makeslice ** main.go:6 0x10b7137 488b442418 mov rax, qword ptr [rsp+0x18] main.go:6 0x10b713c 4889442460 mov qword ptr [rsp+0x60], rax main.go:6 0x10b7141 0f57c0 xorps xmm0, xmm0 main.go:6 0x10b7144 0f11442468 movups xmmword ptr [rsp+0x68], xmm0 ... 在一堆指令中，看到 call $runtime.makeslice 的调用应该是初始化 slice func makeslice(et *_type, len, cap int) unsafe.Pointer { mem, overflow := math.MulUintptr(et.size, uintptr(cap)) if overflow || mem &gt; maxAlloc || len &lt; 0 || len &gt; cap { // NOTE: Produce a &#39;len out of range&#39; error instead of a // &#39;cap out of range&#39; error when someone does make([]T, bignumber). // &#39;cap out of range&#39; is true too, but since the cap is only being // supplied implicitly, saying len is clearer. // See golang.org/issue/4085. mem, overflow := math.MulUintptr(et.size, uintptr(len)) if overflow || mem &gt; maxAlloc || len &lt; 0 { panicmakeslicelen() } panicmakeslicecap() } return mallocgc(mem, et, true) } makeslice 最后返回真正值存储的数组域的内存地址，函数中 uintptr() 是什么呢？ println(uintptr(0), ^uintptr(0)) // 0 18446744073709551615 为什么按位异或后是这个数? var c int = 1 println(^c, ^uint64(0)) // -2 18446744073709551615 从这几行代码验证，有符号的1，二进制为：0001，异或后：1110，最高位1是负数，表示-2；uint64二进制：0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000异或后：1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 1111因为无符号的，转换成10进制，就是 2 ^ 64 - 1 = 18446744073709551615。所以，其实^uintptr(0) 就是指当前机器（32位，uint32；64位，uint64）的最大值。我们可以打印下现在的 a (dlv) p a []int len: 1, cap: 0, [0] Slice 扩容=&gt; main.go:7 0x10b7149 eb00 jmp 0x10b714b main.go:7 0x10b714b 488d0dee0d0100 lea rcx, ptr [rip+0x10dee] main.go:7 0x10b7152 48890c24 mov qword ptr [rsp], rcx main.go:7 0x10b7156 4889442408 mov qword ptr [rsp+0x8], rax main.go:7 0x10b715b 0f57c0 xorps xmm0, xmm0 main.go:7 0x10b715e 0f11442410 movups xmmword ptr [rsp+0x10], xmm0 main.go:7 0x10b7163 48c744242003000000 mov qword ptr [rsp+0x20], 0x3 main.go:7 0x10b716c e84f9bf8ff call $runtime.growslice main.go:7 0x10b7171 488b442428 mov rax, qword ptr [rsp+0x28] main.go:7 0x10b7176 488b4c2430 mov rcx, qword ptr [rsp+0x30] main.go:7 0x10b717b 488b542438 mov rdx, qword ptr [rsp+0x38] main.go:7 0x10b7180 4883c103 add rcx, 0x3 main.go:7 0x10b7184 eb00 jmp 0x10b7186 main.go:7 0x10b7186 48c70002000000 mov qword ptr [rax], 0x2 main.go:7 0x10b718d 48c7400803000000 mov qword ptr [rax+0x8], 0x3 main.go:7 0x10b7195 48c7401004000000 mov qword ptr [rax+0x10], 0x4 main.go:7 0x10b719d 4889442460 mov qword ptr [rsp+0x60], rax main.go:7 0x10b71a2 48894c2468 mov qword ptr [rsp+0x68], rcx main.go:7 0x10b71a7 4889542470 mov qword ... 在对 slice 做 append 的时候，其实是调用了 call runtime.growslice，看看做了什么： func growslice(et *_type, old slice, cap int) slice { if cap &lt; old.cap { panic(errorString(&quot;growslice: cap out of range&quot;)) } if et.size == 0 { // append should not create a slice with nil pointer but non-zero len. // We assume that append doesn&#39;t need to preserve old.array in this case. return slice{unsafe.Pointer(&amp;zerobase), old.len, cap} } newcap := old.cap doublecap := newcap + newcap if cap &gt; doublecap { newcap = cap } else { if old.len &lt; 1024 { newcap = doublecap } else { for 0 &lt; newcap &amp;&amp; newcap &lt; cap { newcap += newcap / 4 } if newcap &lt;= 0 { newcap = cap } } } var overflow bool var lenmem, newlenmem, capmem uintptr // Specialize for common values of et.size. // For 1 we don&#39;t need any division/multiplication. // For sys.PtrSize, compiler will optimize division/multiplication into a shift by a constant. // For powers of 2, use a variable shift. switch { case et.size == 1: lenmem = uintptr(old.len) newlenmem = uintptr(cap) capmem = roundupsize(uintptr(newcap)) overflow = uintptr(newcap) &gt; maxAlloc newcap = int(capmem) case et.size == sys.PtrSize: lenmem = uintptr(old.len) * sys.PtrSize newlenmem = uintptr(cap) * sys.PtrSize capmem = roundupsize(uintptr(newcap) * sys.PtrSize) overflow = uintptr(newcap) &gt; maxAlloc/sys.PtrSize newcap = int(capmem / sys.PtrSize) case isPowerOfTwo(et.size): var shift uintptr if sys.PtrSize == 8 { // Mask shift for better code generation. shift = uintptr(sys.Ctz64(uint64(et.size))) &amp; 63 } else { shift = uintptr(sys.Ctz32(uint32(et.size))) &amp; 31 } lenmem = uintptr(old.len) &lt;&lt; shift newlenmem = uintptr(cap) &lt;&lt; shift capmem = roundupsize(uintptr(newcap) &lt;&lt; shift) overflow = uintptr(newcap) &gt; (maxAlloc &gt;&gt; shift) newcap = int(capmem &gt;&gt; shift) default: lenmem = uintptr(old.len) * et.size newlenmem = uintptr(cap) * et.size capmem, overflow = math.MulUintptr(et.size, uintptr(newcap)) capmem = roundupsize(capmem) newcap = int(capmem / et.size) } if overflow || capmem &gt; maxAlloc { panic(errorString(&quot;growslice: cap out of range&quot;)) } var p unsafe.Pointer if et.ptrdata == 0 { // 申请内存 p = mallocgc(capmem, nil, false) // 清除未使用的地址 memclrNoHeapPointers(add(p, newlenmem), capmem-newlenmem) } else { p = mallocgc(capmem, et, true) if lenmem &gt; 0 &amp;&amp; writeBarrier.enabled { bulkBarrierPreWriteSrcOnly(uintptr(p), uintptr(old.array), lenmem) } } // 拷贝大小为 lenmem 个btyes，从old.array到p memmove(p, old.array, lenmem) return slice{p, old.len, newcap} 具体扩容的策略： 如果要申请的容量（cap）大于 2 倍的原容量（old.cap）或者 原容量 &lt; 1024 ，那么newcap = old.cap + old.cap 否则，计算 newcap += newcap / 4，知道 newcap 不小于要申请的容量，如果溢出，newcap = cap（要申请的容量） 扩容完成后就开始根据 t.size 的大小，重新计算地址，其中新 slice 的 len 为原 slice 的 cap (只有 slice 的 len 超过 cap，才需要扩容)。接着申请 capmem 大小的内存，从 old.array 拷贝 lenmem 个 bytes (就是原 slice 整个拷贝，lenmem 就是计算的原切片的大小)到 p。 a := make([]int, 0) a = append(a, 1) println(&quot;1 times:&quot;, len(a), cap(a)) // 1 times: 1 1 a = append(a, 2, 3) println(&quot;2 times:&quot;, len(a), cap(a)) // 2 times: 3 4 a = append(a, 4) println(&quot;3 times:&quot;, len(a), cap(a)) // 3 times: 4 4 可以看出: 如果 append 后的 len 大于 cap 的2倍，即扩大至大于 len 的第一个2的倍数 如果 append 后的 len 大于 cap 且小于 cap 的两倍，cap扩大至2倍 如果 append 后的 len 小于 cap，直接追加 Slice污染使用 slice，也许不知不觉中就会造成一些问题。 a := []int{1, 2, 3, 4, 5} shadow := a[1:3] shadow = append(shadow, 100) fmt.Println(shadow, a) // [2 3 100] [1 2 3 100 5] 结果很意外，但也是符合逻辑。a 的结构体中 array 是指向数组 [1,2,3,4,5]的内存地址，shadow 是指向其中 [2，3] 的内存地址。在向 shadow 增加后，会直接修改真实的数组，间接影响到指向数组的所有切片。所以可以修改上述代码为： a := []int{1, 2, 3, 4, 5} shadow := append([]int{}, a[1:3]...) shadow = append(shadow, 100) fmt.Println(shadow, a) // [2 3 100] [1 2 3 4 5] 如果某个函数的返回值，是上述的这种情况 return a[1:3]，还会造成 [1,2,3,4,5] 锁占用的内存无法释放。 黑魔法知道了 slice 本身是指向真实的数组的指针，在 Golang 中提供了 unsafe 来做指针操作。 a := []int{1, 2, 3, 4, 5} shadow := a[1:3] shadowPtr := uintptr(unsafe.Pointer(&amp;shadow[0])) offset := unsafe.Sizeof(int(0)) fmt.Println(*(*int)(unsafe.Pointer(shadowPtr - offset))) // 1 fmt.Println(*(*int)(unsafe.Pointer(shadowPtr + 2*offset))) // 4 shadowPtr 是 a 的第1个下标的位置，一个 int 在64位机器上是8 bytes，向前偏移1个 offset，是 a 的第0个下标 1；向后偏移2个 offset，是 a 的第3个下标 4。 并发安全slice 是非协程安全的数据类型，如果创建多个 goroutine 对 slice 进行并发读写，会造成丢失。看一段代码 package main import ( &quot;fmt&quot; &quot;sync&quot; ) func main () { a := make([]int, 0) var wg sync.WaitGroup for i := 0; i &lt; 10000; i++ { wg.Add(1) go func(i int) { a = append(a, i) wg.Done() }(i) } wg.Wait() fmt.Println(len(a)) } // 9403 9876 9985 9491 ... 多次执行，每次得到的结果都不一样，总之一定不会是想要的 10000 个。想要解决这个问题，按照协程安全的编程思想来考虑问题，可以考虑使用 channel 本身的特性(阻塞)来实现安全的并发读写。 func main() { a := make([]int, 0) buffer := make(chan int) go func() { for v := range buffer { a = append(a, v) } }() var wg sync.WaitGroup for i := 0; i &lt; 10000; i++ { wg.Add(1) go func(i int) { buffer &lt;- i wg.Done() }(i) } wg.Wait() fmt.Println(len(a)) } // 10000","categories":[],"tags":[{"name":"go","slug":"go","permalink":"http://8090lambert.cn/tags/go/"}]},{"title":"分布式事务原理","slug":"分布式事务原理","date":"2019-04-15T14:36:45.000Z","updated":"2021-05-09T14:50:16.598Z","comments":true,"path":"2019/04/15/分布式事务原理/","link":"","permalink":"http://8090lambert.cn/2019/04/15/分布式事务原理/","excerpt":"","text":"事务的必要性一般提到事务，首先想到的就是 MySQL 的 transaction，但是很多场景，仅仅依靠 MySQL, 还是无法保证业务场景需要的 ACID。以购物场景为例，张三购买物品，账户扣款 100 元的同时，需要保证在下游的会员服务中给该账户增加 100 积分。而扣款的业务和增加积分的业务是在两个不同的应用，正常处理逻辑一般是先扣除100元，然后网络通知积分服务增加100积分。类似这种业务需求，就必须要用分布式事务来保证。如下图： 以上过程会存在3个问题： 账号服务在扣款的时候宕机了，这时候可能扣款成功，也可能扣款失败； 由于网络稳定性无法保证，通知扣积分服务可能失败，但是扣款成功了； 扣款成功，并且通知成功，但是增加积分的时候失败了。 实际上，rocketmq 的事务消息解决的是问题1和问题2这种场景，也就是解决本地事务执行与消息发送的原子性问题。即解决 Producer 执行业务逻辑成功之后投递消息可能失败的场景。 而对于问题3这种场景，rocketmq提供了消费失败重试的机制。但是如果消费重试依然失败怎么办？rocketmq本身并没有提供解决这种问题的办法，例如如果加积分失败了，则需要回滚事务，实际上增加了业务复杂度，而官方给予的建议是：人工解决。RocketMQ目前暂时没有解决这个问题的原因是：在设计实现消息系统时，我们需要衡量是否值得花这么大的代价来解决这样一个出现概率非常小的问题。 事务消息的实现思路和过程RocketMQ 事务消息的设计流程同样借鉴了两阶段提交理论，通过在执行本地事务前后发送两条消息来保证本地事务与发送消息的原子性，过程如下图： 事务消息详细过程说明 Producer发送Half(prepare)消息到broker； half消息发送成功之后执行本地事务； （由用户实现）本地事务执行如果成功则返回commit，如果执行失败则返回roll_back。 Producer发送确认消息到broker（也就是将步骤3执行的结果发送给broker），这里可能broker未收到确认消息，下面分两种情况分析： 如果 broker 收到了确认消息： 如果收到的结果是 commit，则 broker 视为整个事务过程执行成功，将消息下发给Conusmer端消费； 如果收到的结果是 rollback，则 broker 视为本地事务执行失败，broker删除Half消息，不下发给consumer。 如果 broker 未收到了确认消息： broker定时回查本地事务的执行结果；（由用户实现）如果本地事务已经执行则返回commit；如果未执行，则返回rollback；Producer端回查的结果发送给broker；broker接收到的如果是commit，则broker视为整个事务过程执行成功，将消息下发给Conusmer端消费；如果是rollback，则broker视为本地事务执行失败，broker删除Half消息，不下发给consumer。如果broker未接收到回查的结果（或者查到的是unknow），则broker会定时进行重复回查，以确保查到最终的事务结果。 补充：对于过程3，如果执行本地事务突然宕机了（相当本地事务执行结果返回unknow），则和broker未收到确认消息的情况一样处理。 事务消息的使用关于rocketmq事务消息如何使用，最好的学习思路是从github上下载下源码，参考demo示例。这里也以官方的demo讲解如何使用（在demo基础上做了一点修改）。 代码示例为了模拟事务执行的异常场景，这里会模拟发送5条事务消息，前三条（msg-1、msg-2、msg-3）对应的本地事务执行结果为unknow（模拟本地事务执行未知的情况）; 第4条消息（msg-4）返回COMMIT_MESSAGE（模拟本地事务执行成功的情况），第5条消息（msg-5）返回ROLLBACK_MESSAGE（模拟本地事务执行失败的情况）; 对于前三条消息，模拟回查到的本地事务处理结果分别为UNKNOW，COMMIT_MESSAGE，ROLLBACK_MESSAGE。 发送事务的逻辑： public class TransactionProducer { public static void main(String[] args) throws MQClientException, InterruptedException { //事务执行的listener，由用户实现及接口，提供本地事务执行的代码，以及回查本地事务处理结果的逻辑。 TransactionListener transactionListener = new TransactionListenerImpl(); TransactionMQProducer producer = new TransactionMQProducer(&quot;TransactionProducer&quot;); producer.setNamesrvAddr(&quot;localhost:9876&quot;); producer.setTransactionListener(transactionListener); producer.start(); //模拟发送5条消息 for (int i = 1; i &lt; 6; i++) { try { Message msg = new Message(&quot;TransactionTopicTest&quot;, null, &quot;msg-&quot; + i, (&quot;Hello RocketMQ &quot; + i).getBytes(RemotingHelper.DEFAULT_CHARSET)); producer.sendMessageInTransaction(msg, null); Thread.sleep(10); } catch (MQClientException | UnsupportedEncodingException e) { e.printStackTrace(); } } Thread.sleep(Integer.MAX_VALUE); producer.shutdown(); } } 提供本地事务执行以及回查本地事务的逻辑： public class TransactionListenerImpl implements TransactionListener { private AtomicInteger transactionIndex = new AtomicInteger(0); private AtomicInteger checkTimes = new AtomicInteger(0); private ConcurrentHashMap&lt;String, Integer&gt; localTrans = new ConcurrentHashMap&lt;&gt;(); /** * 本地事务的执行逻辑实现 * 模拟5条消息本地事务的处理结果 * @param msg Half(prepare) message * @param arg Custom business parameter * @return */ @Override public LocalTransactionState executeLocalTransaction(Message msg, Object arg) { LocalTransactionState state = null; //msg-4返回COMMIT_MESSAGE if(msg.getKeys().equals(&quot;msg-4&quot;)){ state = LocalTransactionState.COMMIT_MESSAGE; } //msg-5返回ROLLBACK_MESSAGE else if(msg.getKeys().equals(&quot;msg-5&quot;)){ state = LocalTransactionState.ROLLBACK_MESSAGE; }else{ //这里返回unknown的目的是模拟执行本地事务突然宕机的情况（或者本地执行成功发送确认消息失败的场景） state = LocalTransactionState.UNKNOW; //假设3条消息的本地事务结果分别为1，2，3 localTrans.put(msg.getKeys(), transactionIndex.incrementAndGet()); } System.out.println(&quot;executeLocalTransaction:&quot; + msg.getKeys() + &quot;,excute state:&quot; + state +&quot;,current time：&quot; + new Date()); return state; } /** * 回查本地事务的代码实现 * 第1条消息模拟unknow（例如回查的时候网络依然有问题的情况）。 * 第2条消息模拟本地事务处理成功结果COMMIT_MESSAGE。 * 第3条消息模拟本地事务处理失败结果需要回滚ROLLBACK_MESSAGE。 * * @param msg Check message * @return */ @Override public LocalTransactionState checkLocalTransaction(MessageExt msg) { System.out.print(&quot;checkLocalTransaction message key：&quot;+msg.getKeys()+&quot;,current time：&quot; + new Date()); //根据key获取到3条消息本地事务的处理结果(实际业务场景一般是通过获取msg中的消息体数据来确定某条消息的本地事务是否执行成功) Integer status = localTrans.get(msg.getKeys()); if (null != status) { switch (status) { case 1: System.out.println(&quot; check result：unknow ，回查次数：&quot;+checkTimes.incrementAndGet()); //依然无法确定本地事务的执行结果，返回unknow，下次会继续回查结果 return LocalTransactionState.UNKNOW; case 2: //查到本地事务执行成功，返回COMMIT_MESSAGE，producer继续发送确认消息（此逻辑无需自己写，mq本身提供） //或者查到本地事务执行成功了，但是想回滚掉，则这里需要返回ROLLBACK_MESSAGE，同时写回滚的逻辑，实际如何处理根据业务场景而定 System.out.println(&quot; check result：commit message&quot;); return LocalTransactionState.COMMIT_MESSAGE; case 3: //查询到本地事务执行失败，需要回滚消息。 System.out.println(&quot; check result：rollback message&quot;); return LocalTransactionState.ROLLBACK_MESSAGE; } } return LocalTransactionState.COMMIT_MESSAGE; } } 运行结果分析 仔细观察日志输出和romcketmq的控制台，我们可以得出如下结论： msg-4、msg-5消息没有执行回查事务消息的逻辑，是因为msg-4、msg-5在本地执行事务的时候已经返回了确定的事务执行结果，因此msg-4、msg-5不会回查； msg-1、msg-2、msg-3在执行完本地事务10s后，都回查了本地事务的结果； msg-2、msg-3只回查了一次，因为这两条消息在回查的时候已经返回了确切的事务执行结果； msg-1回查了5次，并且间隔为1分钟，因为msg-1在回查的事务状态依然为unknow，因此会反复回查，直到超过了回查的默认次数不再回查; 对比msg-2和msg-4的消息存储时间，msg-4的存储时间恰好是执行本地事务返回的时间，而msg-2的存储时间则恰好是第一次回查事务结果返回的时间; 关键代码如下： public TransactionSendResult sendMessageInTransaction(final Message msg,final TransactionListener tranExecuter, final Object arg){ //1.发送prepare消息 SendResult sendResult = this.send(msg); LocalTransactionState localTransactionState = LocalTransactionState.UNKNOW; Throwable localException = null; switch (sendResult.getSendStatus()) { case SEND_OK: { try { //2.如果prepare消息发送成功，执行TransactionListener的executeLocalTransaction实现，也就是本地事务方法 localTransactionState = tranExecuter.executeLocalTransaction(msg, arg); } catch (Throwable e) { localException = e; } } break; case FLUSH_DISK_TIMEOUT: case FLUSH_SLAVE_TIMEOUT: case SLAVE_NOT_AVAILABLE: localTransactionState = LocalTransactionState.ROLLBACK_MESSAGE; break; default: break; } //3.结束事务，其实就是针对前面发送的prepare消息再发送一条确认消息（这条确认消息包含了本地事务执行的结果，这里可以猜测broker接收到该确认消息和之前的prepare消息必然有比较大的关联） this.endTransaction(sendResult, localTransactionState, localException); } 大致思路是： 发送prepare消息； 执行实现了TransactionListener的executeLocalTransaction方法，也就是执行本地事务的逻辑； 结束事务，将过程2得到的本地事务结果通过发送另外一条确认消息告诉broker； 因此我们这里可以推测：broker必然会根据前后两条消息来确定如何处理该事务消息。 broker端的处理事务消息回查逻辑public class TransactionalMessageCheckService extends ServiceThread { @Override public void run() { //检查间隔，默认一分钟，可配置 long checkInterval = brokerController.getBrokerConfig().getTransactionCheckInterval(); while (!this.isStopped()) { try { //等待一分钟，以实现每一分钟回查需要的事务消息结果 waitPoint.await(interval, TimeUnit.MILLISECONDS); } catch (InterruptedException e) { log.error(&quot;Interrupted&quot;, e); } finally { //处理事务消息回查的核心逻辑方法 brokerController.getTransactionalMessageService().check(timeout, checkMax,this.brokerController.getTransactionalMessageCheckListener()); } } } } public class TransactionalMessageServiceImpl implements TransactionalMessageService { public void check(long transactionTimeout, int transactionCheckMax,AbstractTransactionalMessageCheckListener listener) { //获取到所有的RMQ_SYS_TRANS_HALF_TOPIC消息队列（prepare消息） Set&lt;MessageQueue&gt; msgQueues = transactionalMessageBridge.fetchMessageQueues(&quot;RMQ_SYS_TRANS_HALF_TOPIC&quot;); for (MessageQueue messageQueue : msgQueues) { //从RMQ_SYS_TRANS_OP_HALF_TOPIC消息队列中获取到prepare消息对应的op消息（确认消息） MessageQueue opQueue = getOpQueue(messageQueue); //prepare消息的offset long halfOffset = transactionalMessageBridge.fetchConsumeOffset(messageQueue); //prepare消息 MessageExt msgExt = getHalfMsg(messageQueue, i); //中间会有一堆的逻辑判断用于是否需要回查事务状态。 //例如：是否超过了回查的次数（默认五次）、消息是否已经失效了、对应的op消息是否已经处理了等。 if (isNeedCheck) { //交给线程池异步处理回调查询事务的状态。 listener.resolveHalfMsg(msgExt); } } } } 大概的处理思路是：broker维护一个死循环，每一分钟执行一次，broker通过使用两个内部队列：RMQ_SYS_TRANS_HALF_TOPIC、RMQ_SYS_TRANS_OP_HALF_TOPIC来存储事务消息推进状态，服务端通过比对两个队列的差值来找到尚未提交的超时事务，调用Producer端，用来查询事务处理结果。 Producer端接收broker回查的逻辑//接收broker的回调，回查本地事务情况，进行相应处理 @Override public void checkTransactionState(final String addr, final MessageExt msg,final CheckTransactionStateRequestHeader header) { //处理broker检查本地事务处理情况的回调任务 Runnable request = new Runnable() { @Override public void run() { //执行TransactionListener实现的checkLocalTransaction方法，检查本地事务处理情况。 LocalTransactionState localTransactionState = transactionCheckListener.checkLocalTransaction(message); //将检查本地事务处理情况再次发送给broker。 this.processTransactionState(localTransactionState,group,exception); } //处理本地事务处理的结果反馈 private void processTransactionState(final LocalTransactionState localTransactionState,final String producerGroup,final Throwable exception) { final EndTransactionRequestHeader thisHeader = new EndTransactionRequestHeader(); ... 根据检查到的本地事务执行的不同结果封装成不同的处理类型发送给broker switch (localTransactionState) { case COMMIT_MESSAGE: thisHeader.setCommitOrRollback(MessageSysFlag.TRANSACTION_COMMIT_TYPE); break; case ROLLBACK_MESSAGE: thisHeader.setCommitOrRollback(MessageSysFlag.TRANSACTION_ROLLBACK_TYPE); break; case UNKNOW: thisHeader.setCommitOrRollback(MessageSysFlag.TRANSACTION_NOT_TYPE); break; default: break; } //结果反馈给broker DefaultMQProducerImpl.this.mQClientFactory.getMQClientAPIImpl().endTransactionOneway(brokerAddr,thisHeader,remark,3000); } }; //提交任务到线程池 this.checkExecutor.submit(request); } 大致的处理思路是：Producer端一个线程池维护执行TransactionListener的executeLocalTransaction实现，也就是本地事务方法的任务。将查询到的本地事务结果反馈给broker端，broker来决定对事务消息如何处理。","categories":[],"tags":[{"name":"http","slug":"http","permalink":"http://8090lambert.cn/tags/http/"}]},{"title":"从 Ethernet 到 tcp 包分析","slug":"从Ethernet到tcp","date":"2019-01-27T13:28:41.000Z","updated":"2021-05-09T14:50:16.597Z","comments":true,"path":"2019/01/27/从Ethernet到tcp/","link":"","permalink":"http://8090lambert.cn/2019/01/27/从Ethernet到tcp/","excerpt":"","text":"以太网帧格式 “ 以太网是一种计算机局域网技术。IEEE组织的IEEE 802.3标准制定了以太网的技术标准，它规定了包括物理层的连线、电子信号和介质访问层协议的内容。以太网是目前应用最普遍的局域网技术，取代了其他局域网技术如令牌环、FDDI和ARCNET。” – Wiki百科 从 Xerox 公布的 Ethernet I 发展到现在，有过6种以太帧格式： Ethernet I Ethernet II Ethernet 802.3 raw Ethernet 802.3 SAP 802.3/802.2 LLC 802.3/802.2 SNAP 其中主流应用的是 Ethernet II、802.3/802.2 LLC、802.3/802.2 SNAP 这三种，最常用的是 RFC894 定义，也就是 Ethernet II 的帧格式。 Ethernet II 目标 MAC 地址：6个字节（48位），发送时会先检查目标 MAC 的地址，与当前适配器的物理地址是否一致，不一致就丢弃； 源 MAC 地址：6个字段（48位），发送帧的网络适配器物理地址 类型：上层协议的类型，常见的有，0x0800 表示是 IPV4 协议，0x0806 表示是 ARP 协议，0x86DD 表示是 IPV6 协议，更多详见 数据报文：最小 46 字节，最大 1500 字节（MTU） 802.3/802.2 LLC DASP：1个字节，目的服务访问点 SSAP：1个字节，源服务访问点将 Ethernet II 帧头的类型字段替换为帧长度，并且因为新增加了 DASP, SSAP，Control这三个各占1字节的字段，报文的长度也调整为：43~1497，它们三个字段作为 LLC 的头 802.3/802.2 SNAP 类型：2个字节，不同于 Ethernet II 的类型字段 OUI ID：3个字节，通常都为 0数据报文变为：38~1492字节 Ethernet 帧，从最上层（应用层）发送的数据单元（PDU），每经过一层，都会把上层整个的 PDU 作为下层 PDU 的 data 域，然后加上自己的协议头；接受端，同下而上的层层拆掉每层的头部。了解了这些，我们尝试抓包具体分析每个字段 Tcp 报文$ tcpdump -i eth1 port 9527 -s 0 -w ./target9527.cap用 wireshark 打开抓到的二进制报文，如图所示： 建立连接Frame 1，表示第1帧，源ip和目的ip分别是：172.24.31.67 和 10.96.77.128，都是内网ip。 type：0x0800 表示 IPV4 源 MAC 地址：04:25:c5:83:f5:64 目标设备mac地址：5e:38:57:10:84:d9 Flags：0x4000，没有拆包，如果请求报文大于 MTU，会拆多次发送 Times to live：ttl，存活时间，数据包每经过一个三层路由器设备时，ttl域的值减1，当其存活次数为0时，便会取消数据包的转发。ttl，默认值是64，如下图，经过 14 次到达目标ip，所有64-13=51 SYN：1，表示 请求及建立连接，包括剩下的两次握手请求包 发送数据包从 Frame 4 至 Frame 7 是建立连接后，发送具体请求的数据包。首先，发送了 HTTP 协议的 GET 请求，收到请求后回复了ACK。具体看下： 在 GET 请求时，设置了标志位 ACK 和 PSH，PSH 是告诉接收端，立即交由应用层处理而不必等到Recv socket buffer写满 接收端回复 ACK 和 Seq number，接收端和发送端同样会再回复一个PSH标记的包，要求立即处理 最后，应用层通过 HTTP 协议回复报文，在回复报文时，这里要注意还有个标志位，在报文中，FIN=1，表示在返回的同时请求关闭连接 断开连接tcp 连接是双工的，所以任何建立连接的双方都可以发起关闭连接的请求。自己之前面试也总喜欢问这些问题，看看候选人到底理解的是否透彻，可是大多数都不怎么清楚。言归正传，剩下的 Frame 8 至 Frame 11 是回复详情的 ACK 和 端开连接的 tcp 包。 Frame 8 和 Frame 9，分别回复的是，Frame 6 PSH标志位的请求和 Frame 7 的 HTTP 请求 Frame 10 是连接另一边发起了关闭连接的请求，标志位 FIN=1 Frame 11 是发起关闭请求方，回复上一个 FIN=1 的 ACK 请求包 四次挥手全部结束。有同学可以会比较疑惑，不是应该是4次请求吗，这里只有三次。解释下这个问题：因为服务端在响应HTTP请求时，因为知道自己已经发送完全部数据，所以在响应包里加上了四次挥手中的第一次 FIN=1 的请求。","categories":[],"tags":[{"name":"http","slug":"http","permalink":"http://8090lambert.cn/tags/http/"}]},{"title":"epoll究竟快在哪","slug":"epoll究竟快在哪","date":"2018-12-06T07:51:15.000Z","updated":"2021-05-09T14:50:16.596Z","comments":true,"path":"2018/12/06/epoll究竟快在哪/","link":"","permalink":"http://8090lambert.cn/2018/12/06/epoll究竟快在哪/","excerpt":"","text":"为什么 epoll 这么快epoll是多路复用IO(I/O Multiplexing)中的一种方式,但是仅用于linux2.6以上内核,在开始讨论这个问题之前,先来解释一下为什么需要多路复用IO. 以一个生活中的例子来解释. 假设你在大学中读书,要等待一个朋友来访,而这个朋友只知道你在A号楼,但是不知道你具体住在哪里,于是你们约好了在A号楼门口见面.如果你使用的阻塞IO模型来处理这个问题,那么你就只能一直守候在A号楼门口等待朋友的到来,在这段时间里你不能做别的事情,不难知道,这种方式的效率是低下的. 现在时代变化了,开始使用多路复用IO模型来处理这个问题.你告诉你的朋友来了A号楼找楼管大妈,让她告诉你该怎么走.这里的楼管大妈扮演的就是多路复用IO的角色. 进一步解释select和epoll模型的差异. select版大妈做的是如下的事情:比如同学甲的朋友来了,select版大妈比较笨,她带着朋友挨个房间进行查询谁是同学甲,你等的朋友来了,于是在实际的代码中,select版大妈做的是以下的事情: int n = select(&amp;readset,NULL,NULL,100); for (int i = 0; n &gt; 0; ++i) { if (FD_ISSET(fdarray[i], &amp;readset)) { do_something(fdarray[i]); --n; } } epoll版大妈就比较先进了,她记下了同学甲的信息,比如说他的房间号,那么等同学甲的朋友到来时,只需要告诉该朋友同学甲在哪个房间即可,不用自己亲自带着人满大楼的找人了.于是epoll版大妈做的事情可以用如下的代码表示: n = epoll_wait(epfd,events,20,500); for(i=0;i&lt;n;++i) { do_something(events[n]); } 在epoll中,关键的数据结构epoll_event定义如下: typedef union epoll_data { void *ptr; int fd; __uint32_t u32; __uint64_t u64; } epoll_data_t; struct epoll_event { __uint32_t events; /* Epoll events */ epoll_data_t data; /* User data variable */ } 可以看到,epoll_data是一个union结构体,它就是epoll版大妈用于保存同学信息的结构体,它可以保存很多类型的信息:fd,指针,等等.有了这个结构体,epoll大妈可以不用吹灰之力就可以定位到同学甲. 别小看了这些效率的提高,在一个大规模并发的服务器中,轮询IO是最耗时间的操作之一.再回到那个例子中,如果每到来一个朋友楼管大妈都要全楼的查询同学,那么处理的效率必然就低下了,过不久楼底就有不少的人了. 对比最早给出的阻塞IO的处理模型, 可以看到采用了多路复用IO之后, 程序可以自由的进行自己除了IO操作之外的工作, 只有到IO状态发生变化的时候由多路复用IO进行通知, 然后再采取相应的操作, 而不用一直阻塞等待IO状态发生变化了. 从上面的分析也可以看出,epoll比select的提高实际上是一个用空间换时间思想的具体应用. 深入理解epoll的实现原理开发高性能网络程序时，windows开发者们言必称iocp，linux开发者们则言必称epoll。大家都明白epoll是一种IO多路复用技术，可以非常高效的处理数以百万计的socket句柄，比起以前的select和poll效率高大发了。我们用起epoll来都感觉挺爽，确实快，那么，它到底为什么可以高速处理这么多并发连接呢？ 先简单回顾下如何使用C库封装的3个epoll系统调用吧。 int epoll_create(int size) int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event) int epoll_wait(int epfd, struct epoll_event *events,int maxevents, int timeout) 使用起来很清晰，首先要调用epoll_create建立一个epoll对象。参数size是内核保证能够正确处理的最大句柄数，多于这个最大数时内核可不保证效果。epoll_ctl可以操作上面建立的epoll，例如，将刚建立的socket加入到epoll中让其监控，或者把 epoll正在监控的某个socket句柄移出epoll，不再监控它等等。epoll_wait在调用时，在给定的timeout时间内，当在监控的所有句柄中有事件发生时，就返回用户态的进程。 从上面的调用方式就可以看到epoll比select/poll的优越之处：因为后者每次调用时都要传递你所要监控的所有socket给select/poll系统调用，这意味着需要将用户态的socket列表copy到内核态，如果以万计的句柄会导致每次都要copy几十几百KB的内存到内核态，非常低效。而我们调用epoll_wait时就相当于以往调用select/poll，但是这时却不用传递socket句柄给内核，因为内核已经在epoll_ctl中拿到了要监控的句柄列表。 所以，实际上在你调用epoll_create后，内核就已经在内核态开始准备帮你存储要监控的句柄了，每次调用epoll_ctl只是在往内核的数据结构里塞入新的socket句柄。 对于linux操作系统而言，一切皆文件。所以，epoll向内核注册了一个文件系统，用于存储上述的被监控socket。当你调用epoll_create时，就会在这个虚拟的epoll文件系统里创建一个file结点。当然这个file不是普通文件，它只服务于epoll。 epoll在被内核初始化时（操作系统启动），同时会开辟出epoll自己的内核高速cache区，用于安置每一个我们想监控的socket，这些socket会以红黑树的形式保存在内核cache里，以支持快速的查找、插入、删除。这个内核高速cache区，就是建立连续的物理内存页，然后在之上建立slab层，简单的说，就是物理上分配好你想要的size的内存对象，每次使用时都是使用空闲的已分配好的对象。 static int __init eventpoll_init(void) { ... ... /* Allocates slab cache used to allocate &quot;struct epitem&quot; items */ epi_cache = kmem_cache_create(&quot;eventpoll_epi&quot;, sizeof(struct epitem), 0, SLAB_HWCACHE_ALIGN|EPI_SLAB_DEBUG|SLAB_PANIC, NULL, NULL); /* Allocates slab cache used to allocate &quot;struct eppoll_entry&quot; */ pwq_cache = kmem_cache_create(&quot;eventpoll_pwq&quot;, sizeof(struct eppoll_entry), 0, EPI_SLAB_DEBUG|SLAB_PANIC, NULL, NULL); ... ... epoll的高效就在于，当我们调用epoll_ctl往里塞入百万个句柄时，epoll_wait仍然可以飞快的返回，并有效的将发生事件的句柄给我们用户。这是由于我们在调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，在内核cache里建了个红黑树用于存储以后epoll_ctl传来的socket外，还会再建立一个list链表，用于存储准备就绪的事件，当epoll_wait调用时，仅仅观察这个list链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。所以，epoll_wait非常高效。 而且，通常情况下即使我们要监控百万计的句柄，大多一次也只返回很少量的准备就绪句柄而已，所以，epoll_wait仅需要从内核态copy少量的句柄到用户态而已，如何能不高效？！ 那么，这个准备就绪list链表是怎么维护的呢？当我们执行epoll_ctl时，除了把socket放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后就来把socket插入到准备就绪链表里了。 如此，一颗红黑树，一张准备就绪句柄链表，少量的内核cache，就帮我们解决了大并发下的socket处理问题。执行epoll_create时，创建了红黑树和就绪链表，执行epoll_ctl时，如果增加socket句柄，则检查在红黑树中是否存在，存在立即返回，不存在则添加到树干上，然后向内核注册回调函数，用于当中断事件来临时向准备就绪链表中插入数据。执行epoll_wait时立刻返回准备就绪链表里的数据即可。 最后看看epoll独有的两种模式LT和ET。无论是LT和ET模式，都适用于以上所说的流程。区别是，LT模式下，只要一个句柄上的事件一次没有处理完，会在以后调用epoll_wait时次次返回这个句柄，而ET模式仅在第一次返回。 这件事怎么做到的呢？当一个socket句柄上有事件时，内核会把该句柄插入上面所说的准备就绪list链表，这时我们调用epoll_wait，会把准备就绪的socket拷贝到用户态内存，然后清空准备就绪list链表，最后，epoll_wait干了件事，就是检查这些socket，如果不是ET模式（就是LT模式的句柄了），并且这些socket上确实有未处理的事件时，又把该句柄放回到刚刚清空的准备就绪链表了。所以，非ET的句柄，只要它上面还有事件，epoll_wait每次都会返回。而ET模式的句柄，除非有新中断到，即使socket上的事件没有处理完，也是不会次次从epoll_wait返回的。 扩展阅读（epoll与之前其他相关技术的比较）Linux提供了select、poll、epoll接口来实现IO复用，三者的原型如下所示，本文从参数、实现、性能等方面对三者进行对比。 int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); int poll(struct pollfd *fds, nfds_t nfds, int timeout); int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout); select、poll、epoll_wait参数及实现对比 ： select的第一个参数nfds为fdset集合中最大描述符值加1，fdset是一个位数组，其大小限制为__FD_SETSIZE（1024），位数组的每一位代表其对应的描述符是否需要被检查； select的第二三四个参数表示需要关注读、写、错误事件的文件描述符位数组，这些参数既是输入参数也是输出参数，可能会被内核修改用于标示哪些描述符上发生了关注的事件。所以每次调用select前都需要重新初始化fdset。 select对应于内核中的sys_select调用，sys_select首先将第二三四个参数指向的fd_set拷贝到内核，然后对每个被SET的描述符调用进行poll，并记录在临时结果中（fdset），如果有事件发生，select会将临时结果写到用户空间并返回；当轮询一遍后没有任何事件发生时，如果指定了超时时间，则select会睡眠到超时，睡眠结束后再进行一次轮询，并将临时结果写到用户空间，然后返回。 select返回后，需要逐一检查关注的描述符是否被SET（事件是否发生）。 poll与select不同，通过一个pollfd数组向内核传递需要关注的事件，故没有描述符个数的限制，pollfd中的events字段和revents分别用于标示关注的事件和发生的事件，故pollfd数组只需要被初始化一次。 poll的实现机制与select类似，其对应内核中的sys_poll，只不过poll向内核传递pollfd数组，然后对pollfd中的每个描述符进行poll，相比处理fdset来说，poll效率更高。 poll返回后，需要对pollfd中的每个元素检查其revents值，来得指事件是否发生。 epoll通过epoll_create创建一个用于epoll轮询的描述符，通过epoll_ctl添加/修改/删除事件，通过epoll_wait检查事件，epoll_wait的第二个参数用于存放结果。 epoll与select、poll不同，首先，其不用每次调用都向内核拷贝事件描述信息，在第一次调用后，事件信息就会与对应的epoll描述符关联起来。另外epoll不是通过轮询，而是通过在等待的描述符上注册回调函数，当事件发生时，回调函数负责把发生的事件存储在就绪事件链表中，最后写到用户空间。 epoll返回后，该参数指向的缓冲区中即为发生的事件，对缓冲区中每个元素进行处理即可，而不需要像poll、select那样进行轮询检查。","categories":[],"tags":[{"name":"http","slug":"http","permalink":"http://8090lambert.cn/tags/http/"}]},{"title":"对比IO和IO多路复用模型","slug":"对比IO和IO多路复用模型","date":"2018-12-04T06:51:15.000Z","updated":"2021-05-09T14:50:16.599Z","comments":true,"path":"2018/12/04/对比IO和IO多路复用模型/","link":"","permalink":"http://8090lambert.cn/2018/12/04/对比IO和IO多路复用模型/","excerpt":"","text":"IO 模式 和 IO 多路复用用户空间和内核空间&emsp;&emsp;现在操作系统都采用虚拟寻址，处理器先产生一个虚拟地址，通过地址翻译成物理地址（内存的地址），再通过总线的传递，最后处理器拿到某个物理地址返回的字节。 &emsp;&emsp;对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方）。操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操心系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。针对linux操作系统而言，将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），供内核使用，称为内核空间，而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF），供各个进程使用，称为用户空间。 &emsp;&emsp;补充：地址空间就是一个非负整数地址的有序集合。如{0,1,2…}。 进程上下文切换&emsp;&emsp;为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换（也叫调度）。因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的。 &emsp;&emsp;从一个进程的运行转到另一个进程上运行，这个过程中经过下面这些变化： 保存当前进程A的上下文 上下文就是内核再次唤醒当前进程时所需要的状态，由一些对象（程序计数器、状态寄存器、用户栈等各种内核数据结构）的值组成。这些值包括描绘地址空间的页表、包含进程相关信息的进程表、文件表等。 切换页全局目录以安装一个新的地址空间 恢复进程B的上下文 处理B进程逻辑 恢复进程A的上下文 … 可以理解成一个比较耗资源的过程 进程的阻塞&emsp;&emsp;正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态。可见，进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得CPU），才可能将其转为阻塞状态。当进程进入阻塞状态，是不占用CPU资源的。 文件描述符（FD）&emsp;&emsp;文件描述符（File descriptor）是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。&emsp;&emsp;文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。对于UNIX、Linux这样的操作系统的设计初衷就是：一切皆文件。 直接I/O和缓存I/O&emsp;&emsp;缓存 I/O 又被称作标准 I/O，大多数文件系统的默认 I/O 操作都是缓存 I/O。在 Linux 的缓存 I/O 机制中，以write为例，数据会先被拷贝进程缓冲区，在拷贝到操作系统内核的缓冲区中，然后才会写到存储设备中。 缓存I/O的write 直接I/O的write：（省去了拷贝到进程缓冲区这一步） write过程中会有很多次拷贝，知道数据全部写到磁盘。好了，准备知识概略复习了一下，开始探讨IO模式。 I/O模式&emsp;&emsp;对于一次IO访问（这回以read举例），数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的缓冲区，最后交给进程。所以说，当一个read操作发生时，它会经历两个阶段： 等待数据准备 (Waiting for the data to be ready) 将数据从内核拷贝到进程中 (Copying the data from the kernel to the process) 因为这两个阶段，linux系统产生了下面五种网络I/O的模式： 阻塞 I/O（blocking IO） 非阻塞 I/O（nonblocking IO） I/O 多路复用（ IO multiplexing） 信号驱动 I/O（ signal driven IO） 异步 I/O（asynchronous IO）信号驱动在实际中并不常用，所以只讨论其他四种 Block I/O 模型（阻塞I/O）&emsp;&emsp;还是以read为例子 进程发起 read，进行 recvfrom 系统调用； 内核开始第一阶段，准备数据（从磁盘拷贝到缓冲区），进程请求的数据并不是一下就能准备好；准备数据是要消耗时间的； 与此同时，进程阻塞（进程是自己选择阻塞与否），等待数据ing； 直到数据从内核拷贝到了用户空间，内核返回结果，进程解除阻塞。 也就是说，内核准备数据和数据从内核拷贝到进程内存地址这两个过程都是阻塞的。 None-Block I/O 模型（非阻塞I/O）&emsp;&emsp;可以通过设置socket使其变为non-blocking。当对一个non-blocking socket执行读操作时，流程是这个样子： 当用户进程发出 read 操作时，如果 Kernel 中的数据还没有准备好； 那么它并不会 block 用户进程，而是立刻返回一个 error，从用户进程角度讲 ，它发起一个 read 操作后，并不需要等待，而是马上就得到了一个结果； 用户进程判断结果是一个 error 时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦 kernel 中的数据准备好了，并且又再次收到了用户进程的 system call； 那么它马上就将数据拷贝到了用户内存，然后返回。 Non-Block IO的特点是用户进程在内核准备数据的阶段需要不断的主动询问数据好了没有。 Multiplexing I/O (多路复用I/O)&emsp;&emsp;I/O多路复用实际上就是用select, poll, epoll监听多个io对象，当io对象有变化（有数据）的时候就通知用户进程。好处就是单个进程可以处理多个socket。当然具体区别我们后面再讨论，现在先来看下I/O多路复用的流程： 当用户进程调用了 select，那么整个进程会被 block； 而同时，kernel 会“监视”所有 select 负责的 socket； 当任何一个 socket 中的数据准备好了，select就会返回; 这个时候用户进程再调用 read 操作，将数据从 kernel 拷贝到用户进程;&emsp;&emsp;I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select() 函数就可以返回。&emsp;&emsp;这个图和blocking IO的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个system call (select 和 recvfrom)，而blocking IO只调用了一个system call (recvfrom)。但是，用select的优势在于它可以同时处理多个connection。&emsp;&emsp;所以，如果处理的连接数不是很高的话，使用 select/epoll 的 web server 不一定比使用多线程 + 阻塞 IO的 web server 性能更好，可能延迟还更大。select/epoll 的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。在IO multiplexing Model中，实际中，对于每一个 socket，一般都设置成为 none-block，但是，如上图所示，整个用户的 process 其实是一直被 block 的。只不过 process 是被 select 这个函数 block，而不是被 socket IO 给 block (对应 epoll 的就是 epoll_wait() 会 block 住 process)。 Asynchronous I/O（异步 I/O）异步 I/O 是真正做到了非阻塞，流程大概如下： 用户进程发起read操作之后，立刻就可以开始去做其它的事。 而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。 然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。 Reference： https://www.cnblogs.com/wangyao2317072926/p/7918643.html","categories":[],"tags":[{"name":"http","slug":"http","permalink":"http://8090lambert.cn/tags/http/"}]},{"title":"Redis-字典","slug":"Redis-字典","date":"2018-11-29T14:51:47.000Z","updated":"2021-05-09T14:50:16.595Z","comments":true,"path":"2018/11/29/Redis-字典/","link":"","permalink":"http://8090lambert.cn/2018/11/29/Redis-字典/","excerpt":"","text":"Redis 还有一种比较常用的数据类型，字典。C 语言没有这种数据结构，但是 在JAVA 语言中的 map，和 PHP 语言里的 关联数组，就是这种类型，它用来保存 key =&gt; value 的键值对，要求键必须是唯一，在字段中，已知一个 key，对这个的 key 对应 value 做 CRUD操作，都非常方便。 Redis 自己实现了这个结构，并且用它来作 哈希键的底层实现之一，如果 一个哈希类型包含的键值对较多 或者 键值对中的元素都是较长的字符串，就会用 字典 来作为它的实现。数据库的 CRUD 操作也是作用在 字典上。具体的源码在文件 src/dict.h，它的底层使用哈希表，哈希表内部可以拥有多个节点，每个节点上就存储这一个键值对。 // 哈希表 typedef struct dictht { dictEntry **table; // 哈希表数组 unsigned long size; // 哈希表大小，一般是2的指数 unsigned long sizemask; // 哈希表掩码，等于 size - 1 unsigned long used; // 哈希表已有节点数量，used / size 这个是装载因子，比值越大，hash冲突月 } dictht; // 哈希表节点 typedef struct dictEntry { void *key; union { void *val; uint64_t u64; int64_t s64; double d; } v; struct dictEntry *next; } dictEntry; 在 dictht 结构体中 table 数组中的每个元素，都是一个指向哈希表节点 dictEntry 的指针，size 标识dictEntry指针数组，就是 table 数组的长度，它总是2的指数；sizemask 的值为 size - 1， 这个属性和哈希值决定了一个键要被放在 table 数组的哪个索引位置上（通过hash &amp; sizemask计算出），used 的属性则记录了哈希表目前已有节点的数量，size / used 是装在因子，这个值越大，哈希冲突的概率就越高。 这个就是 dict 和 dictEntry 的结构示意： dictEntry 结构是哈希表的节点，其中 key 是键值对的键，而 v 是键值对的值，v 中可以是一个指针、uint64 的整数、uint64的整数或者是一个 double 的浮点数，next 属性是指向下一个哈希表节点 dictEntry 结构体的指针，所谓下一个哈希表节点，就是当哈希键 key 相同的哈希表节点，它们会挨个排列，前一个的 next 指针指向后一个。 Hash键冲突Redis 使用 MurmurHash 算法来计算键的hash值，虽然在效率上和随机性上很好，但是还是会产生 hash冲突。针对这个问题，采用了 拉链法 解决冲突。next 指针，表示两个相同的hash组成的一个链表。包含多个键hash值相同的情况： 每次冲突时，都会从头部插入新的键，时间复杂度保证都是O(1)。 在 dict 结构体里，管理这两个的 dictht typedef struct dict { dictType *type; void *privdata; dictht ht[2]; long rehashidx; /* rehashing not in progress if rehashidx == -1 */ unsigned long iterators; /* number of iterators currently running */ } dict; type 和 privdata 属性是为实现多态字典而设置的，主要是用于不同类型的键值对。其实，type 是指向 dictType 结构的指针，在 dictType 结构体中，保存着一系列的方法： typedef struct dictType { uint64_t (*hashFunction)(const void *key); // 计算哈希值 void *(*keyDup)(void *privdata, const void *key); // 复制键 void *(*valDup)(void *privdata, const void *obj); // 复制值 int (*keyCompare)(void *privdata, const void *key1, const void *key2); // 对比键 void (*keyDestructor)(void *privdata, void *key); // 销毁键 void (*valDestructor)(void *privdata, void *obj); // 销毁值 } dictType; ht 是拥有两个元素的数组，每个都是一个 dictht。一般情况，使用第1个索引位置的哈希表来存储，第二个索引位置的哈希表一般是做 rehash 时使用的。rehashidx 就是表示是否正在做 rehash，如果目前不是正在进行，它的值为 -1。iterators 是当前正在运行的迭代器数目。 typedef struct dictIterator { dict *d; long index; int table, safe; dictEntry *entry, *nextEntry; /* unsafe iterator fingerprint for misuse detection. */ long long fingerprint; } dictIterator; 这是字典的迭代器。d 是指向字典的指针，index 是迭代器当前索引的位置。 Rehash如果哈希表的数据变的越来越多，在装载因子（dictht 中 size / used 的比值）超过预定时，这时，Redis 会对哈希表做 Rehash 操作，它采用的是一种增量式重哈希，就是将rehash的过程，分散到所有对 dict 的增删改查操作中。开始 rehash 后，所有的写入操作，都直接写入 ht[1] 里，同时，会将 ht[0] 哈希表在 rehashidx 上的所有键 rehash 至 ht[1]，直到 ht[0] 中键的个数为 0时，rehash 结束。释放 ht[0], 再将 ht[1] 设为 ht[0], 重新申请一个空白哈希表作为新的 ht[1]。 ##Redis对于字典的操作就是这些，还有一些 API，没有做更深入的研究。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://8090lambert.cn/tags/redis/"}]},{"title":"聊聊504、502和499的故事","slug":"聊聊504、502和499的故事","date":"2018-11-27T14:21:37.000Z","updated":"2021-05-09T14:50:16.599Z","comments":true,"path":"2018/11/27/聊聊504、502和499的故事/","link":"","permalink":"http://8090lambert.cn/2018/11/27/聊聊504、502和499的故事/","excerpt":"","text":"每次工作中碰到 5xx 的 http 状态码，都会比较头疼，又要排查了，每次都是从 nginx 的 error_log 追溯，一直到 php_error_log，确定大概位置，翻代码… 恰巧碰到最近组内在学习 nginx 源码，打算深入研究一下，如果有可能让偶发事件成可控事件，那一定能明白其中的原因，更快定位问题，对日后工作排查，应该会有帮助。 Gateway Time-out 504错误是（网关超时） 服务器作为网关或代理，但是没有及时从上游服务器收到请求。服务器（不一定是 Web 服务器）正在作为一个网关或代理来完成客户（如您的浏览器或我们的 CheckUpDown 机器人）访问所需网址的请求。 为了完成您的 HTTP 请求， 该服务器访问一个上游服务器， 但没得到及时的响应。 这是维基百科上直译过来的一段话，网关超时，可以推测下，对于我们常见的服务模型来说，就是 client 请求到 nginx，php-fpm 未在 nginx 的最大执行时间内返回。试着构造一下： 首先准备环境： Ubuntu 18.04 LTS；nginx version: nginx/1.14.0 (Ubuntu); PHP 7.2.14-1+ubuntu18.04.1+deb.sury.org+1 (cli) 修改 Nginx 中 fastcgi_read_timeout = 3，这个参数是说从 fastcgi 读取数据的最大时间，单位为秒 php 代码重启 nginx，直接在浏览器请求; 和推测的完全吻合，通俗点说，就是 php 在 nginx 超时时间(设置的3s)内未返回，nginx 等不及，就会返回给客户端 504,error_log 如下,2018/11/28s 00:13:05 [error] 3540#3540: *3 upstream timed out (110: Connection timed out) while reading response header from upstream, client: 192.168.10.1, server: mine.test, request: &quot;GET / HTTP/1.1&quot;, upstream: &quot;fastcgi://unix:/var/run/php/php7.2-fpm.sock&quot;, host: &quot;mine.test&quot; Bad Gateway 502状态码是服务器（不一定是Web服务器）作为网关或代理，以满足客户的要求来访问所请求的URL 。此服务器收到无效响应从上游服务器访问履行它的要求。 无效响应 可以理解是下游服务（php-fpm）不可用 还是刚才的环境，这次，停掉 php-fpm 进程 (因为我是多版本共存的，nginx配置的是php7.2的php-fpm，剩下的都是7.1php-fpm的worker进程)现在，访问刚才的url 我们再模拟一种情况：在 php-fpm.conf 中有个参数设置 ; The timeout for serving a single request after which the worker process will ; be killed. This option should be used when the &#39;max_execution_time&#39; ini option ; does not stop script execution for some reason. A value of &#39;0&#39; means &#39;off&#39;. ; Available units: s(econds)(default), m(inutes), h(ours), or d(ays) ; Default Value: 0 request_terminate_timeout = 15 每个 php-fpm 的 worker 进程如果执行时间超过，会被 kill 掉。就是 php-fpm 等待程序的最大时长，修改为 request_terminate_timeout = 3，然后重启 php-fpm，刷新浏览器。发现和刚才的情况一样，也是 nginx 返回给 clint 一个 502 的错误页面。 在 nginx 的 error_log 里会出现这样一条日志：2018/11/28 00:20:43 [error] 3793#3793: *20 connect() to unix:/var/run/php/php7.2-fpm.sock failed (111: Connection refused) while connecting to upstream, client: 192.168.10.1, server: mine.test, request: &quot;GET / HTTP/1.1&quot;, upstream: &quot;fastcgi://unix:/var/run/php/php7.2-fpm.sock:&quot;, host: &quot;mine.test&quot; 同时，access_log 里也会增加一条日志：192.168.10.1 - - [28/Nov/2018:00:20:43 +0800] &quot;GET / HTTP/1.1&quot; 504 594 &quot;-&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36&quot; 这两种情况，有个共同点：就是没有正常的返回给 nginx （不论是php-fpm服务不可用还是php-fpm等待超时，主动断开）。 Client has closed connection499：客户端关闭连接 这个乍一看，是客户端的锅，是它先“动手”，关闭连接的。从标准的 RFC2616 协议中，是没有 499 状态码，是 nginx 自己定义的。 根据字面的意思，这种情况是客户端主动断开连接，或许是服务端在客户端最大等待时间内，没有返回结果，导致客户端等不及。这次我们在终端试试（在写的时候，已用浏览器测试，奈何浏览器作为客户端，等待时间太长了，复现不了）。 服务端还是这行代码： &lt;?php sleep(10); echo &quot;hello world&quot;; 在终端执行，紧接着 command + c 强制终止进程 $ curl http://mine.test 从 nginx 的 access_log 中查看日志发现：192.168.10.1 - - [28/Nov/2018:14:42:26 +0800] &quot;GET / HTTP/1.1&quot; 499 0 &quot;-&quot; &quot;curl/7.54.0&quot; 因为客户端主动断开，在下游的 nginx 就会有一条 499 的日志。在普通的 web 应用中很少见，但是在微服务下，跨部门跨组之间的调用，都是 http 请求，如果下游的服务不够稳定，这种状态码一般很多。上游要保证，在连接等待的时候，必须有合理的超时时间，不能因为下游服务挂掉了，而拖垮自己，导致整个服务雪崩。 说到底，这三种状态码（不考虑php-fpm服务不可用时的502），都是 nginx 、php-fpm 和 client 中的两者之间发生超时，只要能够分清是谁timeout，就能准确的判断会发生的情况。","categories":[],"tags":[{"name":"http","slug":"http","permalink":"http://8090lambert.cn/tags/http/"}]},{"title":"Redis-整数集合","slug":"Redis-整数集合","date":"2018-11-11T15:25:02.000Z","updated":"2021-05-09T14:50:16.595Z","comments":true,"path":"2018/11/11/Redis-整数集合/","link":"","permalink":"http://8090lambert.cn/2018/11/11/Redis-整数集合/","excerpt":"","text":"在 Redis 中，如果有用过它的 set 集合，底层的实现整数集合。整数集合，就是只能包括整数值的元素，我们试着创建一个整数集合 它的结构很简单，具体的源码在 src/intset.h 文件中 typedef struct intset { uint32_t encoding; // 编码类型 uint32_t length; // 元素数量 int8_t contents[]; // 保存元素数组 } intset; 在 contents 数组中，集合的每个元素，都对应着数组的一个元素，他们按照从小到大的顺序，有序的排列着，并且每个元素在数组中都是唯一的。length 记录了集合中现有元素的数量，可以 O（1）的方式，每次快速返回 contents 数组的数量。encoding 属性是值，contents 数组中的每个元素，都是以什么类型存的，共有三种类型： /* Note that these encodings are ordered, so: * INTSET_ENC_INT16 &lt; INTSET_ENC_INT32 &lt; INTSET_ENC_INT64. */ #define INTSET_ENC_INT16 (sizeof(int16_t)) // int16, -2^15 ~ 2^15 - 1 #define INTSET_ENC_INT32 (sizeof(int32_t)) // int32, -2^31 ~ 2^31 - 1 #define INTSET_ENC_INT64 (sizeof(int64_t)) // int64, -2^63 ~ 2^63 - 1 一个拥有5个int16类型的整数集合 集合的类型转换当我们需要往现有集合中增加插入一个整数的时候，如果要添加的整数的大小不满足现有集合的编码类型，则需要对现有集合的每个元素转换类型。具体步骤： 根据新元素的类型，扩展整数集合的空间大小； 将现有元素类型，转换为集合中最大数所属类型，保持原有顺序，再添加新元素至合适位置； 修改 contents 数组长度 length 和 编码类型 encoding 例如，有一个集合，包含3个元素，分别是 1，2，3，集合的 encoding 属性为 INTSET_ENC_INT16，所占空间为 3 * 16 = 48 位，6个字节。现在要将65535 添加到集合中，按照上面说的步骤： 首先，因为 65535 超过 INTSET_ENC_INT16，属于 INTSET_ENC_INT32 范围内，所以，需要对空间进行重新分配，新的空间需要 4 * 32 = 128 位，16个字节； 接着，从 96 ~ 127 位，留给最后一个 65535 元素，剩下的三个元素，从 95 位开始，至 96 - 32 = 64 位，保存3，从 63 位开始至 64 - 32 = 32 位，保存2，剩下的0 至 31 位，保存 1，最后将要插入的 65535 放入最先预留的 96 - 127 位； 将 length 改为4， encoding 改为 INTSET_ENC_INT32 至此，类型转换就完成了。整数集合，不会将高位的 encoding，改为低位，如果说，因为增加元素 encoding 的类型改为高位，即使后来再删除元素，encoding 始终就是增加时修改的值。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://8090lambert.cn/tags/redis/"}]},{"title":"Redis-跳跃表","slug":"Redis-跳跃表","date":"2018-11-02T14:04:20.000Z","updated":"2021-05-09T14:50:16.595Z","comments":true,"path":"2018/11/02/Redis-跳跃表/","link":"","permalink":"http://8090lambert.cn/2018/11/02/Redis-跳跃表/","excerpt":"","text":"有一种查询效率特别高的有序的数据结构，跳跃表（SkipList），这种结构，在Redis和levelDB中，都有用到。其实是在有序链表的基础上进行扩展，丰富了在链表中查找制定值，需要 O（N）的时间复杂度的问题，它在查询时，能做到最快 O（1），平均 O（logN），最坏 O（N）的复杂度。 一般，要求查询效率高的结构，都会想到平衡树，但是树比跳跃表更复杂，但是效率，未必有跳跃表高。许多时候可以用跳表来取代平衡树。 在 Redis 中，跳跃表作为 有序集合（ZSET）的底层实现之一和集群节点内部结构。源码在 src/server.h 中 typedef struct zskiplist { struct zskiplistNode *header, *tail; // 头节点指针和尾节点指针 unsigned long length; // 节点数量（不包括头节点） int level; // 所有节点，层数最高的节点的层数 } zskiplist; /* ZSETs use a specialized version of Skiplists */ typedef struct zskiplistNode { sds ele; // 数据值 double score; // 分值 struct zskiplistNode *backward; // 后退指针 struct zskiplistLevel { // 每个节点的层 struct zskiplistNode *forward; // 每层前进指针 unsigned int span; // 距离下个相同层的节点的跨度 } level[]; } zskiplistNode; 作者 antirez 指出 Redis 跳跃表与一般跳跃表的不同 a) this implementation allows for repeated scores. // 允许分值重复b) the comparison is not just by key (our ‘score’) but by satellite data. // 对比的时候不仅比较分值还比较对象的值c) there is a back pointer, so it’s a doubly linked list with the back pointers being only at “level 1”. // 有一个后退指针，即在第一层实现了一个双向链表，允许后退遍历 表头节点是一个拥有32个level的 zskiplistNode 结构，正向遍历跳表，就是从表头的某层开始，一般层的数量越多，查询的效率就会越高，每次插入一个新跳表节点时，会随机生成一个 1 ~ 32 的层数（对应表头节点，最高32层），从索引位置 0 ~ 31。 跳跃表的查找当我们要查找一个分值为46的节点时，普通的链表只能从头至尾循环查找，对于跳跃表，它每次从 level 最高的层开始查找。 首先，从 zskiplist 的头结点找到最高 level 的节点，55 大于 要找的46，根据层的 backward 指针后退至 L3 节点； L3 节点的分值为 21 小于 46 ，则从 55 的下一层开始后退到 L2 节点； L2 节点的分值 37 小于 46， 则从 L2 节点的下一层前进至 L1 节点； L1 节点的分值为 46，查找成功 少量的数据，在做这种对比的时候，优势并不明显，如果数据量特别多，跳过的元素数量将会非常可观，效率的提升也会非常明显。 当跳跃表的多个元素分值（score）相同时，会按照成员对象的字典顺序进行排序，成员对象小的节点排在靠近表头的位置，成员对象大的节点排在靠近表尾的位置。 结语跳跃表是特别高效的节点查找方式，在工作中，在自己的程序设计时，如果有对已排序的数据做查找时，它是比树更简单的一种思路。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://8090lambert.cn/tags/redis/"}]},{"title":"详解Laravel中的依赖注入和IoC","slug":"详解Laravel中的依赖注入和IoC","date":"2017-10-14T13:44:30.000Z","updated":"2021-05-09T14:50:16.600Z","comments":true,"path":"2017/10/14/详解Laravel中的依赖注入和IoC/","link":"","permalink":"http://8090lambert.cn/2017/10/14/详解Laravel中的依赖注入和IoC/","excerpt":"","text":"作为开发者，我们一直在尝试通过使用设计模式和尝试新的健壮型框架来寻找新的方式来编写设计良好且健壮的代码。在本篇文章中，我们将通过 Laravel 的 IoC 组件探索依赖注入设计模式，并了解它如何改进我们的设计。 依赖注入依赖注入一词是由 Martin Fowler 提出的术语，它是将组件注入到应用程序中的一种行为。就像 Ward Cunningham 说的: 依赖注入是敏捷架构中关键元素。 让我们来看一个例子： class UserProvider{ protected $connection; public function __construct(){ $this-&gt;connection = new Connection; } public function retrieveByCredentials( array $credentials ){ $user = $this-&gt;connection -&gt;where( &#39;email&#39;, $credentials[&#39;email&#39;]) -&gt;where( &#39;password&#39;, $credentials[&#39;password&#39;]) -&gt;first(); return $user; } } 如果你要测试或者维护这个类，你必须访问数据库的实例来进行一些查询。为了避免必须这样做，你可以将此类与其他类进行 解耦 ，你有三个选项之一，可以将 Connection 类注入而不需要直接使用它。 将组件注入类时，可以使用以下三个选项之一： 构造方法注入class UserProvider{ protected $connection; public function __construct( Connection $con ){ $this-&gt;connection = $con; } ... Setter 方法注入同样，我们也可以使用 Setter 方法注入依赖关系： class UserProvider{ protected $connection; public function __construct(){ ... } public function setConnection( Connection $con ){ $this-&gt;connection = $con; } ... 接口注入interface ConnectionInjector{ public function injectConnection( Connection $con ); } class UserProvider implements ConnectionInjector{ protected $connection; public function __construct(){ ... } public function injectConnection( Connection $con ){ $this-&gt;connection = $con; } } 当一个类实现了我们的接口时，我们定义了 injectConnection 方法来解决依赖关系。 优势现在，当测试我们的类时，我们可以模拟依赖类并将其作为参数传递。每个类必须专注于一个特定的任务，而不应该关心解决它们的依赖性。这样，你将拥有一个更专注和可维护的应用程序。 如果你想了解更多关于 DI 的信息，Alejandro Gervassio 在 本系列 文章中对其进行了广泛而专业的介绍，所以一定要去读这些文章。那么，什么又是 IoC 呢？IoC （控制反转）不需要使用依赖注入，但它可以帮助你有效的管理依赖关系。 控制反转Ioc 是一个简单的组件，可以更加方便地解析依赖项。你可以将对象形容为容器，并且每次解析类时，都会自动注入依赖项。 Laravel Ioc当你请求一个对象时， Laravel Ioc 在解决依赖关系的方式上有些特殊：我们使用一个简单的例子，将在本文中改进它。SimpleAuth 类依赖于 FileSessionStorage ，所以我们的代码可能是这样的： class FileSessionStorage{ public function __construct(){ session_start(); } public function get( $key ){ return $_SESSION[$key]; } public function set( $key, $value ){ $_SESSION[$key] = $value; } } class SimpleAuth{ protected $session; public function __construct(){ $this-&gt;session = new FileSessionStorage; } } //创建一个 SimpleAuth $auth = new SimpleAuth(); 这是一种经典的方法，让我们从使用构造函数注入开始。 class SimpleAuth{ protected $session; public function __construct( FileSessionStorage $session ){ $this-&gt;session = $session; } } 现在我们创建一个对象： $auth = new SimpleAuth( new FileSessionStorage() ); 现在我想使用 Laravel Ioc 来管理这一切。因为 Application 类继承自 Container 类，所以你可以通过 App 门面来访问容器。 App::bind( &#39;FileSessionStorage&#39;, function(){ return new FileSessionStorage; }); bind 方法第一个参数是要绑定到容器的唯一 ID ，第二个参数是一个回调函数每当执行 FileSessionStorage 类时执行，我们还可以传递一个表示类名的字符串，如下所示。Note:如果你查看 Laravel 包时，你将看到绑定有时会分组，比如（ view, view.finder……）。假设我们将会话存储转换为 Mysql 存储，我们的类应该类似于： class MysqlSessionStorage{ public function __construct(){ //... } public function get($key){ // do something } public function set( $key, $value ){ // do something } } 现在我们已经更改了依赖项，我们还需要更改 SimpleAuth 构造函数，并将新对象绑定到容器中！ 高级模块不应该依赖于低级模块，两者都应该依赖于抽象对象。 抽象不应该依赖于细节，细节应该取决于抽象。 Robert C. Martin 我们的 SimpleAuth 类不应该关心我们的存储是如何完成的，相反它更应该关注于消费的服务。因此，我们可以抽象实现我们的存储： interface SessionStorage{ public function get( $key ); public function set( $key, $value ); } 这样我们就可以实现并请求 SessionStorage 接口的实例： class FileSessionStorage implements SessionStorage{ public function __construct(){ //... } public function get( $key ){ //... } public function set( $key, $value ){ //... } } class MysqlSessionStorage implements SessionStorage{ public function __construct(){ //... } public function get( $key ){ //... } public function set( $key, $value ){ //... } } class SimpleAuth{ protected $session; public function __construct( SessionStorage $session ){ $this-&gt;session = $session; } } 如果我们使用 App::make(&#39;SimpleAuth&#39;) 通过容器解析 SimpleAuth类，容器将会抛出 BindingResolutionException ，尝试从绑定解析类之后，返回到反射方法并解析所有依赖项 Uncaught exception &#39;Illuminate\\Container\\BindingResolutionException&#39; with message &#39;Target [SessionStorage] is not instantiable.&#39; 容器正试图将接口实例化。我们可以为该接口做一个具体的绑定。 App:bind( &#39;SessionStorage&#39;, &#39;MysqlSessionStorage&#39; ); 现在每次我们尝试从容器解析该接口时，我们会得到一个 MysqlSessionStorage 实例。如果我们想要切换我们的存储服务，我们只要变更一下这个绑定。 Note:如果你想要查看一个类是否已经在容器中被绑定，你可以使用 App::bound(&#39;ClassName&#39;) ，或者可以使用 App::bindIf(&#39;ClassName&#39;) 来注册一个还未被注册过的绑定。 Laravel Ioc 也提供 App::singleton(&#39;ClassName&#39;, &#39;resolver&#39;) 来处理单例的绑定。你也可以使用 App::instance(&#39;ClassName&#39;, &#39;instance&#39;) 来创建单例的绑定。如果容器不能解析依赖项就会抛出 ReflectionException ，但是我们可以使用 App::resolvingAny(Closure) 方法以回调函数的形式来解析任何指定的类型。 Note:如果你为某个类型已经注册了一个解析方式 resolvingAny 方法仍然会被调用，但它会直接返回 bind 方法的返回值。 小贴士 这些绑定写在哪儿：如果只是一个小型应用你可以写在一个全局的起始文件 global/start.php 中，但如果项目变得越来越庞大就有必要使用 Service Provider 。 测试:当需要快速简易的测试可以考虑使用 php artisan tinker ，它十分强大，且能帮你提升你的 Laravel 测试流程。 Reflection API：PHP 的 Reflection API 是非常强大的，如果你想要深入 Laravel Ioc 你需要熟悉 Reflection API ，可以先看下这个 教程 来获得更多的信息。 Reference： https://www.sitepoint.com/dependency-injection-laravels-ioc/","categories":[],"tags":[{"name":"php","slug":"php","permalink":"http://8090lambert.cn/tags/php/"}]},{"title":"Redis-链表","slug":"Redis-链表","date":"2017-07-23T14:21:37.000Z","updated":"2021-05-09T14:50:16.596Z","comments":true,"path":"2017/07/23/Redis-链表/","link":"","permalink":"http://8090lambert.cn/2017/07/23/Redis-链表/","excerpt":"","text":"链表是一种基本的数据结构，它是由一系列的节点组成，每个节点上都包括有两部分数据：一个是存储数据元素的数据域，一个是存储下一个节点地址的指针域。新增节点时，可以做到O(1)的复杂度。链表一般分为：单向链表、双向链表、循环链表。Redis 的链表，属于双向链表。 链表中节点结构体在文件 src/adlist.h 如下： typedef struct listNode { struct listNode *prev; struct listNode *next; void *value; } listNode; 其中 prev 指向当前节点的上一个节点，next 指向当前节点的下一个节点；value 用来存储当前节点的数据，其中 prev 和 next 共同组成了链表的指针域，value 则是它的数据域，因为是 void *，所以可以在节点中存储任意的数据类型。 多个 listNode 通过 prev 和 next 就可以组成一个双向链表。但是 Redis 还是决定用一个结构体来管理链表。这是它的结构体： typedef struct list { listNode *head; // 表头节点 listNode *tail; // 表尾节点 void *(*dup)(void *ptr); // 节点复制函数 void (*free)(void *ptr); // 节点释放函数 int (*match)(void *ptr, void *key); // 节点对比函数 unsigned long len; // 链表长度 } list; head、tail 和 len 就不多做介绍了，dup、free 和 match 主要是实现多态链表的函数，因为 listNode 的 value 可以存储任意类型的数据，在复制、释放或者比较时，对于不同的类型，需要有不同的操作，所以，在生成链表时，需要对应的set每种类型的操作（复制、释放和对比）API。在 src/adlist.h 64行开始，有6个宏定义的方法，作为 setter 和 getter。 #define listSetDupMethod(l,m) ((l)-&gt;dup = (m)) #define listSetFreeMethod(l,m) ((l)-&gt;free = (m)) #define listSetMatchMethod(l,m) ((l)-&gt;match = (m)) #define listGetDupMethod(l) ((l)-&gt;dup) #define listGetFree(l) ((l)-&gt;free) #define listGetMatchMethod(l) ((l)-&gt;match) 对于链表结构，获取前一个节点和后一个节点的复杂度都是O(1)，因为有 len 属性，所以获取链表长度复杂度也是O(1)。在网上找了些文章，在 list 的实现，主要就用到了链表，其实可以想到，因为 list 本身就是一个链表的结构。 aslist.h 文件中还有一个结构体 typedef struct listIter { listNode *next; int direction; } listIter; 这个是 list 的迭代器，next 指针指向下个节点，direction 是定义迭代的方向，AL_START_HEAD 和 AL_START_TAIL 分别对应从头部 -&gt; 尾部的方向、尾部 -&gt; 头部的方向。相关的方法有： listIter *listGetIterator(list *list, int direction) // 初始化一个 链表迭代器 listNode *listNext(listIter *iter) // 迭代器 next 方法 void listReleaseIterator(listIter *iter); // 迭代器释放 void listRewind(list *list, listIter *li); // 迭代器恢复指针至表头位置 void listRewindTail(list *list, listIter *li); // 迭代器恢复指针至表尾位置","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://8090lambert.cn/tags/redis/"}]},{"title":"Redis-SDS的实现","slug":"Redis-SDS的实现","date":"2017-07-21T07:58:47.000Z","updated":"2021-05-09T14:50:16.594Z","comments":true,"path":"2017/07/21/Redis-SDS的实现/","link":"","permalink":"http://8090lambert.cn/2017/07/21/Redis-SDS的实现/","excerpt":"","text":"Redis 的作者 antirez 在前些日子，通过博客文章《 The first release candidate of Redis 4.0 is out 》发布了 redis 4.0 版本。在网上看到了一些文章，对于4.0新特性的介绍： Lazyfree，之前的版本，在对一个较大的key执行删除时，会造成 redis-server 阻塞，现在可以使用 UNLINK 异步删除， FLUSHDB 和 FLUSHALL 都新增了 ASYNC 选项，支持在后台线程进行； LFU，新添加了 Last Frequently Used 缓存驱逐策略，具体见 antirez的另一篇文章； 内存命令，新添加了一个 MEMORY 命令， 可以用于内存使用情况； 混合 RDB-AOF 持久化格式，之前大部分的做法，都是同时开启 RDB 和 AOF 两种持久化，这种模式会在 AOF rewrite 文件里同时包含 RDB 格式的内容和 AOF 格式的内容，可以发生问题时迅速载入数据（RDB 优点），还能快速的生成重写文件 更多的可以查看 https://raw.githubusercontent.com/antirez/redis/4.0/00-RELEASENOTES 有了这么几个优化点，带着好奇心，决定看看新版本的源码和之前相比有什么不同。 新版本的 SDS我们都知道，sds 是 Redis 用来表示字符串键值对，在3.0版本中，sds的结构体是这样子的: struct sdshdr { int len; // 记录 buf 数组中已使用字节的数量，等于SDS所保存字符串的长度 int free; // 记录 buf 数组中未使用字节的数量 char buf[]; // 字节数组，用于保存字符串 } 新版本的sds根据保存字符串长度的不同，分别对应5种结构体： /* Note: sdshdr5 is never used, we just access the flags byte directly. * However is here to document the layout of type 5 SDS strings. */ struct __attribute__ ((__packed__)) sdshdr5 { unsigned char flags; /* 3 lsb of type, and 5 msb of string length */ char buf[]; }; struct __attribute__ ((__packed__)) sdshdr8 { uint8_t len; /* used */ uint8_t alloc; /* excluding the header and null terminator */ unsigned char flags; /* 3 lsb of type, 5 unused bits */ char buf[]; }; struct __attribute__ ((__packed__)) sdshdr16 { uint16_t len; /* used */ uint16_t alloc; /* excluding the header and null terminator */ unsigned char flags; /* 3 lsb of type, 5 unused bits */ char buf[]; }; struct __attribute__ ((__packed__)) sdshdr32 { uint32_t len; /* used */ uint32_t alloc; /* excluding the header and null terminator */ unsigned char flags; /* 3 lsb of type, 5 unused bits */ char buf[]; }; struct __attribute__ ((__packed__)) sdshdr64 { uint64_t len; /* used */ uint64_t alloc; /* excluding the header and null terminator */ unsigned char flags; /* 3 lsb of type, 5 unused bits */ char buf[]; }; 新的结构体，其中 len 表示字符串的长度（不包含结尾’\\0’的空字符）；alloc 表示为字符串分配的空间，flags 中最低三个bit，表示类型， buf 存储着具体的字符串 常数复杂度获取字符串长度sds 和 C 字符串都是以一个空字符 “\\0” 结尾。C 字符串本身不记录长度，要想得到长度，必须遍历计算，时间复杂度 O(N)，而 sds 结构体中的 len 记录了本身的长度，所以获取一个 sds 字符串的长度的复杂度是 O(1)。 sds 在每次初始化或者长度有变化时，相应的 api 都会有这样的计算，可以直接使用而无需关心是怎么样做的，有兴趣的可以看源码的 src/sds.h 中的 sdslen 方法（86行） 杜绝缓冲区的溢出在做字符串拼接的时候，在 C 语言里，比如执行 strcat(char *hello, const char *world ) 拼接两个字符串时，在内存中与 hello 紧邻的还有另一个字符串 “hello1”，假定未为 hello 分配足够多的内存，在 strcat 执行后，hello 的数据溢出到 “hello1”，导致其中 “hello” 被 “world” 在不知情的情况下被替换掉，这种情况是非常危险的… sds 在字符串修改时，在拼接之前，会先检查 sds 的空间是否满足，如果不满足的话，会自动（通过调用 sdsMakeRoomFor）修改至所需的大小，然后才执行实际的修改，这样保证了新的字符串不会因为溢出而污染其他相邻内存上的数据。 空间分配策略C 字符串在每次增加或者缩短字符串时，程序总是要对这个 C 字符串的数组进行一次内存重新分配： 如果是增长字符串，那么在执行这个操作之前，需要通过内存重新分配来扩展底层数组的大小，否则就会造成 缓冲区溢出 如果是缩短字符串，那么在执行这个操作之后，需要通过内存重新分配来释放字符串不再使用的那部分空间，否则就会造成 内存泄露 每一次内存分配，都是比较耗时的操作，涉及到复杂的算法和内存的IO，如果出于性能的考虑，就要更可能少的去做这样的事情。 作者的初衷，相信也是将 Redis 定位成更高效，性能更好的NoSQL。为了优化 这种缺陷，sds做了预分配和惰性释放来分别应对这两种内存重新分配的情况。 空间预分配 空间预分配主要作用于 sds 字符串增长操作，通过 API sdsMakeRoomFor 来重新计算分配空间，具体规则在 src/sds.c 210行： if (newlen &lt; SDS_MAX_PREALLOC) newlen *= 2; else newlen += SDS_MAX_PREALLOC; 总结一下就是：如果新的字符串大小 小于 1MB，那么会分配 2倍的新的字符串的大小，即 newlen *= 2，buf 的大小则为 newlen * 2 + 1，一字节用来存结尾空字符 “\\0”；如果新字符串的长度 大于 1MB，那么会分配 新字符串大小 + 1MB，即 newlen += SDS_MAX_PREALLOC。 这样做的话，会将原本增长N次，便会重新进行N次内存分配，变成了，最多会重新分配内存N次，就是说，如果一次预分配后的大小，满足 sds 修改后的大小，就不需要再对内存重新分配，直接增长即可。 空间惰性释放 空间惰性释放是指缩短 sds 时，不是立即使用内存重新分配来回收缩短后多出来的空间，而是继续保留，以便将来使用。以 sdstrim 这个 api 为例，看下它的源码 sds sdstrim(sds s, const char *cset) { char *start, *end, *sp, *ep; size_t len; sp = start = s; ep = end = s+sdslen(s)-1; while(sp &lt;= end &amp;&amp; strchr(cset, *sp)) sp++; while(ep &gt; sp &amp;&amp; strchr(cset, *ep)) ep--; len = (sp &gt; ep) ? 0 : ((ep-sp)+1); if (s != sp) memmove(s, sp, len); s[len] = &#39;\\0&#39;; sdssetlen(s,len); return s; } 只做了 memmove 操作，并没有真正重新分配内存，倘若此时，需要执行 sdscat，拼接上一个较短字符串，就可以直接使用，避免了再次分配内存。 如果有需要，必须使用释放掉这部分内存，可以调用 sdsRemoveFreeSpace 来真正释放掉。 二进制安全我们都知道，C 字符串除了在末尾的空字符外，其他位置不允许有空字符，否则最先被读到的空字符，就会被误认为是字符串结尾。这样限制了只能存储纯文本内容，不能保存二进制数据。 sds 就不会有这样的要求，它可以满足你存储任何数据，即使字符串中间有空字符也不受影响，因为它有 len 来表示字符串是否结尾。sds 所有 API 都是二进制安全的，和 C 字符串一样，是以空字符结尾，因此可以复用 C 字符串的一些函数。 以上就是我对 sds 字符串的认识，觉得作者真的是非常用心的设计，既遵循了规范，并且还有很好的优化点在里面，越来越有兴趣继续研究下去了~","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://8090lambert.cn/tags/redis/"}]},{"title":"PHP序列化的漏洞","slug":"PHP序列化的漏洞","date":"2017-02-18T09:04:19.000Z","updated":"2021-05-09T14:50:16.593Z","comments":true,"path":"2017/02/18/PHP序列化的漏洞/","link":"","permalink":"http://8090lambert.cn/2017/02/18/PHP序列化的漏洞/","excerpt":"","text":"0.前言对象的序列化和反序列化作用就不再赘述,php中序列化的结果是一个php自定义的字符串格式,有点类似json. 我们在任何语言中设计对象的序列化和反序列化都需要解决几个问题 把某个对象序列化之后,序列化的结果有自描述的功能(从序列化的结果中知道这个对象的具体类型,知道类型还不够,当然还需要知道这个类型所对应具体的值). 时间性能问题:在某些性能敏感的场景下,对象序列化就不能拖后腿,例如:高性能服务(我经常使用protobuf来序列化). 空间性能问题:序列化之后的结果不能太长,比如内存中一个int对象,序列化之后数据长度变成了10倍int的长度,那这个序列化算法是有问题的. 本文仅仅从php代码角度来解释php中序列化和反序列化的过程.,记住一点序列化和反序列化操作的仅仅是对象的数据,这一点有面向对象开发经验的都应该容易理解. 1.序列化serialize和反序列化方法unserializephp原生提供了对象序列化功能. 用起来也非常简单,就两个接口. class fobnn { public $hack_id; private $hack_name; public function __construct($name,$id) { $this-&gt;hack_name = $name; $this-&gt;hack_id = $id; } public function print() { echo $this-&gt;hack_name.PHP_EOL; } } $obj = new fobnn(&#39;fobnn&#39;,1); $obj-&gt;print(); $serializedstr = serialize($obj); //通过serialize接口序列化 echo $serializedstr.PHP_EOL;; $toobj = unserialize($serializedstr);//通过unserialize反序列化 $toobj-&gt;print(); fobnn O:5:&quot;fobnn&quot;:2:{s:7:&quot;hack_id&quot;;i:1;s:16:&quot;fobnnhack_name&quot;;s:5:&quot;fobnn&quot;;} fobnn 看到第二行的输出,这个字符串就是序列化的结果,这个结构其实很容读懂,可以发现是通过对象名称/成员名称来映射的,当然不同访问权限的成员序列化之后的标签名称略有不同. 根据我上面讲到的3个问题,那么我们可以来看看 1.自描述功能 O:5:&quot;fobnn&quot;:2 其中o就表示了 object 类型,且类型名称为fobnn, 采用这种格式,后面的2表示了有2个成员对象. 关于成员对象,其实也是同一套子描述,这是一个递归的定义. 自描述的功能主要是通过字符串记录对象和成员的名称来实现. 2.性能问题 php序列化的时间性能本文就不分析了,详见后面,但序列化结果其实类似json/bson定义的协议,有协议头,协议头说明了类型,协议体则说明了类型所对应的值,并不会对序列化结果进行压缩. 2.反序列化中的魔术方法对应上述说的第二个问题,其实php中也有解决方法,一种是通过魔术方法,第二种则是自定义序列化函数.先来介绍下魔术方法 sleep 和 wakeup http://php.net/manual/en/language.oop5.magic.php#object.sleephttp://php.net/manual/en/language.oop5.magic.php#object.wakeup class fobnn { public $hack_id; private $hack_name; public function __construct($name,$id) { $this-&gt;hack_name = $name; $this-&gt;hack_id = $id; } public function print() { echo $this-&gt;hack_name.PHP_EOL; } public function __sleep() { return array(&quot;hack_name&quot;); } public function __wakeup() { $this-&gt;hack_name = &#39;haha&#39;; } } $obj = new fobnn(&#39;fobnn&#39;,1); $obj-&gt;print(); $serializedstr = serialize($obj); echo $serializedstr.PHP_EOL;; $toobj = unserialize($serializedstr); $toobj-&gt;print(); fobnn O:5:&quot;fobnn&quot;:1:{s:16:&quot;fobnnhack_name&quot;;s:5:&quot;fobnn&quot;;} haha 在序列化之前会先调用 __sleep 返回的是一个需要序列化的成员名称数组,通过这样我们就可以控制需要序列化的数据,案例中我只返回了hack_name,可以看到结果中只序列化了hack_name成员. 在序列化完成之后,会跳用 __wakeup 在这里我们可以做一些后续工作,例如重连数据库之类的. 3.自定义Serializable接口自定义序列化接口 http://php.net/manual/en/class.serializable.php interface Serializable { abstract public function serialize() abstract public function unserialize(string $serialized) } 通过这个接口我们可以自定义序列化和反序列化的行为,这个功能主要可以用来自定义我们的序列化格式. class fobnn implements Serializable { public $hack_id; private $hack_name; public function __construct($name,$id) { $this-&gt;hack_name = $name; $this-&gt;hack_id = $id; } public function print() { echo $this-&gt;hack_name.PHP_EOL; } public function __sleep() { return array(&#39;hack_name&#39;); } public function __wakeup() { $this-&gt;hack_name = &#39;haha&#39;; } public function serialize() { return json_encode(array(&#39;id&#39; =&gt; $this-&gt;hack_id ,&#39;name&#39;=&gt;$this-&gt;hack_name )); } public function unserialize($var) { $array = json_decode($var,true); $this-&gt;hack_name = $array[&#39;name&#39;]; $this-&gt;hack_id = $array[&#39;id&#39;]; } } $obj = new fobnn(&#39;fobnn&#39;,1); $obj-&gt;print(); $serializedstr = serialize($obj); echo $serializedstr.PHP_EOL;; $toobj = unserialize($serializedstr); $toobj-&gt;print(); fobnn C:5:&quot;fobnn&quot;:23:{{\"id\":1,\"name\":\"fobnn\"}} fobnn 当使用了自定义序列化接口之后,我们的魔术方法就没用了. 4.PHP动态类型和PHP反序列化既然上文中提到的自描述功能,那么序列化结果中保存了对象的类型,且php是动态类型语言,那么我们就可以来做个简单的实验. class fobnn { public $hack_id; public $hack_name; public function __construct($name,$id) { $this-&gt;hack_name = $name; $this-&gt;hack_id = $id; } public function print() { var_dump($this-&gt;hack_name); } } $obj = new fobnn(&#39;fobnn&#39;,1); $obj-&gt;print(); $serializedstr = serialize($obj); echo $serializedstr.PHP_EOL;; $toobj = unserialize($serializedstr); $toobj-&gt;print(); $toobj2 = unserialize(&quot;O:5:\\&quot;fobnn\\&quot;:2:{s:7:\\&quot;hack_id\\&quot;;i:1;s:9:\\&quot;hack_name\\&quot;;i:12345;}&quot;); $toobj2-&gt;print(); 我们修改hack_name反序列化的结果为int类型，i:12345，结果如下： string(5) “fobnn”O:5:”fobnn”:2:{s:7:”hack_id”;i:1;s:9:”hack_name”;s:5:”fobnn”;}string(5) “fobnn”int(12345) 可以发现,对象成功序列化回来了!并且可以正常工作!. 当然php的这种机制提供了灵活多变的语法,但也引入了安全风险. 后续继续分析php序列化和反序列化特性带来的安全问题.","categories":[],"tags":[{"name":"php","slug":"php","permalink":"http://8090lambert.cn/tags/php/"}]},{"title":"PHP的魔术方法：__clone()","slug":"PHP的魔术方法：-clone","date":"2016-07-16T06:59:14.000Z","updated":"2021-05-09T14:50:16.594Z","comments":true,"path":"2016/07/16/PHP的魔术方法：-clone/","link":"","permalink":"http://8090lambert.cn/2016/07/16/PHP的魔术方法：-clone/","excerpt":"","text":"最近在写代码的时候，碰到需要copy对象，在 php.net 看到这段话： $copy_of_object = clone $object; 当对象被复制后，PHP 5 会对对象的所有属性执行一个浅复制（shallow copy）。所有的引用属性 仍然会是一个指向原来的变量的引用。 赋值在PHP中， 对象间的赋值操作实际上是引用操作 （事实上，绝大部分的编程语言都是如此! 主要原因是内存及性能的问题) $bob = new Boy(10); $jack = $bob; $jack-&gt;age++; echo $jack-&gt;age; // 11 echo $bob-&gt;age; // 11 浅克隆其实 $bob 和 $jack 指针都是指向同一个内存区，所以，修改任意一个对象，另外一个对象也会随之变化。如果我们不希望对象是 reference 方式的复制，这就需要用到 clone 关键字。 $school = new School(); $jack = new Boy(10); $school-&gt;FirstGrade($jack); $school2 = clone $school; $jack-&gt;age++; echo $school-&gt;firstGrader-&gt;age; // 11 echo $school2-&gt;firstGrader-&gt;age; // 11, 克隆对象中的对象 (引用属性) 也被修改 如果想要对象中的属性完全复制(指克隆对象和源对象指向两个不同的内存区) 深克隆class School { public $grader; /** * @param $grader */ public function grade($grader) { $this-&gt;grader = $grader; } public function __clone() { $this-&gt;grader = clone $this-&gt;grader; } } $school = new School(); $jack = new Boy(10); $school-&gt;grade($jack); $school2 = clone $school; $jack-&gt;age++; echo $school-&gt;grader-&gt;age; // 11 echo $school2-&gt;grader-&gt;age; // 10 此时，克隆的对象未发生变化。 总结浅克隆：变量类型为标量(int，string, bool, array)为值传递，对象类型为引用传递。深克隆：所有元素或属性均完全复制，与源对象无关，也就是说所有对于新对象的修改都不会影响到源对象。 Tips在PHP里，只需要 $cloneObject = unserialize(serialize($object)) 对源对象做 序列化-&gt;反序列化 的操作，便可以得到深克隆的对象。（会有性能损失和安全问题，下一章会说对象序列化的安全问题）","categories":[],"tags":[{"name":"php","slug":"php","permalink":"http://8090lambert.cn/tags/php/"}]},{"title":"Hello World","slug":"hello-world","date":"2016-07-13T05:52:06.000Z","updated":"2021-05-09T14:50:16.597Z","comments":true,"path":"2016/07/13/hello-world/","link":"","permalink":"http://8090lambert.cn/2016/07/13/hello-world/","excerpt":"","text":"","categories":[],"tags":[]}]}